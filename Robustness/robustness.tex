\chapter{Improving Adversarial Robustness} \label{chapter:robustness}
\textit{``Concentrate on your strength and strongly build your strength whilst others concentrate on your weakness!''} -Ernest Agyemang Yeboah 

\section{Chapter Overview}
The ability of a classifier to recognize unknown inputs is important for many classification-based systems~\cite{OOD15}. Although modern DNN architectures are efficient predictive models, they often unable to identify why and when their predictions might be incorrect~\cite{OOD3} as the relation between learning confidence and neural networks provides an intuitive production~\cite{OOD3}. This problem occurs when we train and use a model without knowing it's internal working principle and without knowing if it necessary learns concepts to mirror human cognition. Given that a model was evaluated w.r.t the accuracy measures, often it tend to work on average on unseen data. Often we do not know the reason behind such lower performance and consequently, we ignore the reasoning as long as it performs moderately well. 

\hspace*{3.5mm} Even though we cannot specify how a model works, at least we can always identify the samples that are wrongly predicted. Using a brute-force approach, it is possible to find small variations of a specific samples that change the model’s behaviour~(i.e., predictions). Although trivial variations are expected, if they contain unwanted noise or content moderation, such samples becomes so ``adversarial examples''~(AEx). Adversarial examples are used to introduce different types of attacks on the models either to fool them or to expose their internals.
Therefore, it is desirable that a deployed machine learning~(ML) model is capable of detecting such anomalous inputs~(i.e., AEx)~\cite{OOD5}. To ensure a model is robust to adversaries and behaves as intended in real-life scenario, both proactive~(e.g., adversarial retraining) and reactive measures~(e.g., identification of adversarial attack and stop the inferencing) are necessary. In this chapter, different adversarial attacks are introduced on the models, including content moderation, numeric data moderation, and out-of-distribution~(OOD), before assessing the robustness against each scenario. 

\section{Introduction}
In a typical ML pipeline, splitting the available data into training, validation, and test sets is common: a model is trained on the train set, hyperparameters are validated on the validation set, and the trained model is evaluated on the test set. However, often, it is difficult to asses how would a model perform on the test set in a real-world scenario, e.g., in a clinical setting. Besides, if we have multiple models trained and out of which we need to decide which one to deploy in a clinical setting, how would we decide? Suppose, we have 2 trained models $X$ and $Y$. If model $X$ performs better than model $Y$ on the test set, does that mean model $X$ would also be more suitable in a clinical setting? Statistical learning theory can answer this question through some general assumptions. If the test set represents the population to large extent and coming from the same distributions and if the data in the clinical setting is also coming from the same distributions, we can relate the test set error to the real-world error with a high probability. 

\hspace*{3.5mm} However, oftentimes deployed ML models are vulnerable against adversarial attacks. Since the diagnostic decision provided by a clinical decision support system~(DSS) is critical and wrong decisions are not acceptable in any case, it is essential to ensure that the a clinical DSS backed by a model is robust to adversaries and behaves as intended in real-life treatment diagnosis purposes. Bendale et al.~\cite{OOD18} show that it is easy to generate images that humans would never classify as a particular object class, yet networks classify such images with high confidence as that given class. This makes the adversarial images look almost identical to original clean images, but with small perturbations. Although a human could easily distinguish the adversarial MNIST digits~\cite{yuan2019adversarial}, often barely recognizable by humans if trivial amounts of noises are added into an image. An adversarial image sample on ImageNet is shown in \cref{fig:fgsm_example}, where the perturbation~(middle) is generated by adding an imperceptibly small vector whose elements are equal to the sign of the elements of the gradient of the cost function w.r.t the clean image of a panda~(left). The adversarial image is then classified as a gibbon~(right). 

\hspace*{3.5mm} Therefore, even a very minor perturbation is not only enough to misguide an image classifier, i.e., it is very easy to perform such a trivial attack and make the classifier fool. Consequently, the user might get a response of an incorrect image label, which tend the classifier to make wrong predictions. 
Let's consider another image classification problem such as dog vs. cat classifier. Suppose, we have a  model trained on millions of images of dogs and cats. Now in the adversarial scenario, the classification task can be described as follows: a user inputs an image of elephant image to get the prediction of the class label. There is a high possibility that a user will get a response of an incorrect image label, i.e., either dog or cat, forcing the classifier to make a wrong prediction. This way, even it is possible to fool the most robust deep neural network~(DNN) architecture. It becomes more extreme if the test data comes from another distribution than the training set, i.e., OOD, then even the statistical learning theory fails relate the errors unless we provide the model with additional information such as domain knowledge. Consequently, in case of OOD scenario, how the model would perform on a new distribution is not guaranteed. 

\begin{figure}
    \centering
    \includegraphics[scale=0.8]{images/panda_adversary.png}
    \caption[An example of adversarial attack and it's consequence]{An example of adversarial attack and it's consequence. As shown, even minor noises can fool a robust classifier. The adversarial image is generated with FGSM method by Goodfellow et al.~\cite{goodfellow2014explaining}. Left: a clean panda image, middle: added perturbation, right: adversarial image classified as a gibbon}
    \label{fig:fgsm_example}
    \vspace{-4mm}
\end{figure}

\hspace*{3.5mm} Moreover, there is a close intersection between adversarial robustness and model interpretations~\cite{bhatt2020explainable,sharma2019certifai}. The closest adversarial example should perturb `fragile' features, enabling the model to fit to robust features, i.e., class-discriminating feature~\cite{bhatt2020explainable}. Often, discriminative models offer very limited performance guarantees when trained on a dataset that is not generated by the same process as the training distribution, e.g., OOD~\cite{OOD1}. Further, the use of larger and more complex inputs in DNN magnifies the difficulty of distinguishing between anomalous and in-distribution~(ID)~\cite{OOD5}. Such adversarial attacks are perhaps more critical in healthcare, especially when AI-guided systems is used to provide diagnosis aid to a doctor, e.g., in the case of an OOD data point, a learning algorithm often encounter times not only makes a wrong or erroneous prediction but also can come up with a deadly error in clinical diagnosis~\cite{OOD1}. 

\hspace*{3.5mm} In the previous chapter, several predictive models were trained~(i.e., VGG16, $MCAE_{slr}$, and $MCAE_{lrc}$) to predict cancer types based on genomics data. Now, assuming that we have a less complex CNN model trained on gene expression~(GE) data and assuming that the model shows 90\% confidence at predicting different cancer types, when evaluated on a sufficiently large test set. Once the model is deployed and ready for inferencing, making prediction for a single instance is trivial: given an image representation of a GE sample, the model predicts the cancer type as BRCA with 90\% probability. How confident are we that the input to the model is really a GE sample and not a copy number variation~(CNV) sample? The model will definitely make a prediction, not most probably it would be a wrong diagnosis. 

\hspace*{3.5mm} However, making a wrong prediction is far better than a garbage prediction. It is well-known that in the presence of an adversary, it would not be wise to trust  what our model says blindly. Even if we assume there are no adversaries, we still would not have certainty that the input image is a GE sample. If we are given an independently sampled set generated or coming from the same distribution, the model is less likely to make wrong~(as well as garbage) prediction on more than a certain number of samples with a high probability. 
Determining whether an input belongs to the population distribution of the training data~(i.e., ID sample), the model should be robust enough. 

\hspace*{3.5mm} Therefore, in a larger extent, in order to ensure an explainable model is robust to adversaries and behaves as intended in real-life treatment diagnosis, both proactive~(e.g., adversarial retraining) and reactive measures~(e.g., identification of possible adversarial attack and stop the inferencing) are necessary. More specifically, if a sample is of OOD, the prediction is unreliable as the error cannot be bounded. 
Since we are emphasizing not only on high precision in clinical diagnosis, the model itself have to be able to detect OOD kind of adversaries, so that it is robust to such adversaries can lead to more human interpretable features by providing counterfactual explanations. In this chapter, we introduce different types of adversarial attacks on our model, including content moderation, numeric data moderation, and OOD, followed by assessing the robustness against these scenario. 

%The rest of the chapter is structured as follows: \cref{chapter_6:rw} covers some related works on improving adversarial robustness of the ML models and summarize their potential limitations. \Cref{chapter_6:mm} describes the overall approach, including the detail of the data preparation before the network construction and training. \Cref{chapter_6:results} demonstrates the experiment results and discusses key findings of the study. Finally, \cref{chapter_6:conclusion} provides some explanations of the importance and relevance of the study reported, highlights the limitations and discuss some future works before concluding the chapter. 

\section{Related Work} \label{chapter_6:rw}
Since we apply content moderation (on numeric data) and OOD adversarial attacks, we limited our discussion on related work covering these two aspects only. Although some of the discussed works are not directly applicable for our cancer decision support system, they still provide the foundations to come up with effective adversarial robustness in our approach. Nevertheless, usually AEx are almost identical to normal samples, we treat attacks with AEx and OOD as two different types of attacks. 

\subsection{Attacks with adversarial examples}
Till date numerous approaches have been proposed on creating adversarial samples and some of them are already defeated by a countermeasure in later studies~\cite{yuan2019adversarial}. The concept of AEx was first formulated by Dalvi et al.~\cite{dalvi2004adversarial} as a game between adversary and classifier in which the attack and defense on AEx can be correlate as an iterative game. First gradient-based approaches are earliest approaches to generate AEx and attack linear suppport vector machines~(SVM)~\cite{biggio2013evasion}. Szegedy et al.\cite{szegedy2013intriguing} first introduced the concept of AEx against neural networks, where AEx were generated using a L-BFGS method\footnote{ Broyden–Fletcher–Goldfarb–Shanno is an iterative method for solving an unconstrained nonlinear optimization problem} as follows~\cite{yuan2019adversarial}:

\vspace{-6mm}
\begin{align}
    \begin{array}{cl}
        \min _{x^{\prime}} & c\|\eta\|+J_{\theta}\left(x^{\prime}, l^{\prime}\right) \\
         \text {s.t.} & x^{\prime} \in[0,1]
    \end{array}
    \label{eq:fgsm_aex}
\end{align}

\hspace*{3.5mm} where $c$ is a constant used to approximate values of AEx by linear-searching with $c > 0$, making it very expensive an expensive to find the optimal value for $c$. Subsequently, Goodfellow et al.\cite{goodfellow2014explaining}, proposed a fast method called Fast Gradient Sign Method~(FGSM) to generate AEx. The simplicity and effectiveness of FGSM relies on the fact that it needs to perform one step gradient update along the direction of the sign of gradient at each pixel, where the perturbation is expressed as~\cite{goodfellow2014explaining}: 

\vspace{-6mm}
\begin{align}
    \eta=\epsilon \operatorname{sign}\left(\nabla_{x} J_{\theta}(x, l)\right)
    \label{eq:fgsm_eta}
\end{align}

\hspace*{3.5mm} where $\epsilon$ is the magnitude of the perturbation, which is is computed using back-propagation. The corresponding adversarial example of sample $x$ is $x^{\prime}$, which is calculated as $x^{\prime}=x+\eta$~\cite{yuan2019adversarial}. On the other hand, literature~\cite{rozsa2016adversarial} proposed called called Fast Gradient Sign Method~(FGSM) to improve FGSM. In FGSM, the sign of gradient in FGSM is replaced with the raw gradient: $\eta=\nabla_{x} J(\theta, x, l)$. The effectiveness of FGSM relies in it's agnostic nature on constraints on each pixel, hence AEx can be generated with a larger local difference~\cite{yuan2019adversarial}. Since one-step attack is not only easy to transfer to another domain but also also easy to defend~\cite{yuan2019adversarial}, Dong. Y., et al.~\cite{dong2018boosting}, improve FGSM method in which momentum is applied to generate AEx more iteratively, where the gradients were calculated by:

\vspace{-6mm}
\begin{align}
    \mathbf{g}_{t+1}=\mu \mathbf{g}_{t}+\frac{\nabla_{x} J_{\theta}\left(x_{t}^{\prime}, l\right)}{\left\|\nabla_{x} J_{\theta}\left(x_{t}^{\prime}, l\right)\right\|}.
\end{align}

\hspace*{3.5mm} AEx are then subsequently generated by $x_{t+1}^{\prime}=x_{t}^{\prime} + \epsilon~sign~g_{t+1}$. A later method~\cite{tramer2017ensemble} proved that FGSM with adversarial training is more robust to white-box attacks than to black-box attacks due to gradient masking. RAND-FGSM is a new attack they proposed by adding random when updating the AEx to defeat adversarial training~\cite{yuan2019adversarial}:

\vspace{-6mm}
\begin{align}
    x_{t m p} &=x+\alpha \cdot \operatorname{sign}\left(\mathcal{N}\left({0}^{d}, {I}^{d}\right)\right) \\
    x^{\prime} &=x_{t m p}+(\epsilon-\alpha) \cdot \operatorname{sign}\left(\nabla_{x_{t m p}} J\left(x_{t m p}, l\right)\right),
\end{align}
    
\hspace*{3.5mm} where $\alpha, \epsilon$ are the parameters, $\alpha<\epsilon$.  DeepFool~\cite{moosavi2016deepfool} is another method to generate AEx by finding the closest distance from the original input to the decision boundary. To overcome the non-linearity in high dimensional feature space, iterative attack is performed with a linear approximation: starting from an affine classifier~(AC), it is found that the minimal perturbation for an AC is the distance to the separating affine hyperplane ${F}=\{x:$ $\left.w^{T} x+b=0\right\}$, where the perturbation of an AC $f$ is $\eta^{*}(x)=-\frac{f(x)}{\|w\|_{\text {is }}^{2}} w$~\cite{yuan2019adversarial}. In case of a binary differentiable classifier, an iterative method is applied to approximate the perturbation by considering $f$ linearized around $x_{i}$ at each iteration, where the minimal perturbation is computed as~\cite{yuan2019adversarial}:
 
\vspace{-6mm}
\begin{align}
    \begin{array}{ll}
    \underset{\eta_{i}}{\arg \min } & \left\|\eta_{i}\right\|_{2} \\
    \text {s.t.} & F\left(x_{i}\right)+\nabla F\left(x_{i}\right)^{T} \eta_{i}=0.
    \end{array}
\end{align}

\hspace*{3.5mm} It is extended for multi-class classifier by finding the closest hyperplanes by finding a more general $\ell_{p}$ norm, $p \in[0, \infty)$. Eventually, DeepFool provides less perturbation compared to FGSM~\cite{yuan2019adversarial}.

\subsection{Attacks with OOD examples}
Subsequently, numerous approaches have been proposed to accurately identify OOD, in which many researchers have treated it as anomaly detection problem. Since discriminatively trained neural network classifiers produce reliable predictions only for ID samples, detecting OOD samples is essential. Assuming OOD to be outside the closed boundary of ID, typical neural classifiers do not contain the knowledge of this boundary for OOD detection during inference. Since OOD to be outside the closed boundary of ID, a predictive model is incapable of detecting OOD detection during the inferencing time, several recent approaches have been proposed to embed the logic into the learning algorithm and explicitly training the classifier with OOD samples close to the ID boundary. However, state-of-the-art approaches often fail to cover the entire ID boundary effectively, thereby resulting in a sub-optimal OOD detector. 

\begin{table*}[!ht]
    \caption{different cancer detection methods, data types, and performance }
    \label{table:stateofart2}
    \begin{center}
    \scriptsize
    \vspace{-6mm}
    \begin{tabular}{l|l|l|l}
        \hline
        \textbf{Reference} & \textbf{Approach} & \textbf{Use cases} & \textbf{AUROC/FPR} \\\hline
            Ren et al.~\cite{OOD1} & Likelihood ratios & Imaging \& genomics & 93\%/6.6\% \\\hline % 2018
            Vernekar et al.~\cite{OOD2} & Manifold learning network & MNIST and Fashion-MNIST classification & 99.6\%/0.4\% \\\hline % 2018
            DeVries et al.~\cite{OOD3} & Learning confidence estimates & Imaging & 99.1\%/9.3\% \\\hline % 2018
        	Papadopoulos et al.~\cite{OOD4} & Confidence control & Image \& texts  & 97.81\%/6.1\%  \\\hline  % 2017
            Hendrycks et al.~\cite{OOD5} &  Outlier exposure & Imaging & 89.3\%/9.5\% \\\hline % 2016
            Masana et al.~\cite{OOD6} & Metric learning  & MNIST and SVHN & 75\% \\\hline % 2016
        	Rajana et al.~\cite{20Rajanna} & Deep NN & Prostate cancer & 95\%\\\hline
            Chen et al.~\cite{18Chen} & Shallow NN & Colon cancer  & 84\% \\ \hline % 2015
            Ahmed et al.~\cite{abdel2016breast} & DBN & Breast cancer  & 99\% \\ \hline % 2015
            Zheng et al.~\cite{23Zheng} & K-means/SVM & Breast cancer & 97.38\% \\ \hline % 2014
            Xiaofan et al.~\cite{ding2014application} & Naïve Bayes & Cancer risk & 93\% \\ \hline % 2014
        \end{tabular}
    \end{center}
        \vspace{-6mm}
\end{table*}

\hspace*{3.5mm} Lee et al.~\cite{OOD13} explains that test samples drawn sufficiently far away from the training distribution statistically or adversarially is a fundamental requirement for deploying a model in many real-world ML applications. However, deep neural networks with the softmax classifier are known to produce highly overconfident posterior distributions even for such abnormal samples.  %Ensemble of Leave-Out Classifiers (ELOC
Jie et al.~\cite{OOD1} proposed likelihood ratio method for deep generative models for the OOD detection. Since OOD is heavily affected by population level background statistics, they demonstrated that likelihood ratio method can effectively correct for these confounding background statistics. Vernekar et al.~\cite{OOD2} proposed of an efficient approach to generate OOD samples based on manifold learning network. Then, OOD samples are used to train a classifier with an extra class~(i.e., $n+1$ classes, where ${(n+1)}^{th}$ class represents OOD samples). % for the detection of OOD during the inferencing time. 
They show that only a few OOD samples are sufficient to guide the decision boundary of the classifier to be bounded around the ID regions evidenced by their OOD detection results.

\iffalse
\begin{table}[h]
\centering
    \caption{Summary of recent related methods for OOD detection based on literature~\cite{OOD19}. Model update is how the method modifies the original classifiers. The benchmark results are based on DenseNet trained on CIFAR-100 as ID and TinyImageNet-resized as OOD. }
    \vspace{-2mm}
    \label{table:OODs_papers}
    \scriptsize
    \begin{tabular}{l|l|l|l|l|l} 
        \hline
        \textbf{Reference}  & \textbf{Method} & \textbf{Preprocessing require?} & \textbf{Model update} & \textbf{Data} & \textbf{AUROC} \\ \hline
        Qing Y. et al.~\cite{OOD19} & MDBC  & No & Fine tuning  & Labeled ID + unlabeled data  & 99.6 \\ \hline
        Liang Y. et al.~\cite{OOD13} & ODIN  & Yes & Static  & Labeled ID  & 90.7 \\ \hline
        Dan H. et al.~\cite{OOD18} & ODIN  & No & Static  & Labeled ID  & 71.6 \\ \hline
        Viyas A. et al.~\cite{OOD8} & ELOC  & Yes & Ensemble  & Labeled ID  & 96.3 \\ \hline
    \end{tabular}
\end{table}
\fi 

\hspace*{3.5mm} DeVries T. et al.~\cite{OOD3} shows that after training a DNN using IsoMax, ODD samples can be identified efficiently by simply calculating the entropy of the network's output probabilities. The approach proposed by them is called Entropic Score~(ES). The combined approach~(i.e., IsoMax for training and ES for ODD during inference) they proposed is also found to be fast, accurate, scalable, and unexposed. Based on the Outlier Exposure~(OE) technique, Rajati et al.~\cite{OOD4} proposed a novel loss function Outlier Exposure with Confidence Control~(OECC). They show that efficient optimization of OECC cam achieve state-of-the-art results in OOD detection with OE on both image and text classification tasks without needing much OOD samples. Vyas et al.~\cite{OOD8} proposed an ensemble method for the OOD detection which comprises of an ensemble of classifiers. Each classifier is trained in a self-supervised manner by leaving out a random subset of training data as OOD data and the rest as ID data. Although, sounds promising, the ensemble method based on randomly generated OOD samples mixed with real samples, did not preforms well, mainly because of in efficient way of generating OOD samples. 

\hspace*{3.5mm} Shalev et al.~\cite{OOD10}, proposed to use multiple semantic dense representations instead of sparse representation as the target label. Choi et al.~\cite{OOD11} proposed to scale OOD detection to high-dimensional data is to learn a tractable likelihood approximation of the training distribution, and use it to reject unlikely inputs. However, likelihood models on natural data are themselves susceptible to OOD errors, and even assign large likelihoods to samples from other datasets. Quintanilha  et al.~\cite{OOD12}, show that a simple ensembling of first and second order deep feature statistics can be exploited to effectively differentiate ID and OOD samples. It is observed that the mean and standard deviation within feature maps differs greatly between ID and OOD samples. Kliger et al.~\cite{OOD15} discuss the problem of simultaneous classification and novelty detection by determining whether an input is from the known set of classes and from which specific class, or from an unknown domain. They show a multi-class discriminator trained with a generator that generates samples from a mixture of nominal data distributions performs the best.

\hspace*{3.5mm} Liang et al.~\cite{OOD13} propose OOD image detection in neural networks~(ODIN), a simple and effective method that does not require any change to a pre-trained model. ODIN is based on the observation that using feature scaling and adding small perturbations to the input can separate the softmax score distributions between in- and OOD images, allowing for more effective detection. Based on experiments it is found that ODIN is compatible with diverse network architectures and datasets. Hendrycks et al.~\cite{OOD17} considers two related problems of detecting if an example is misclassified or OOD. A simple baseline that utilizes probabilities from softmax distributions can be found effective in computer vision, NLP, and automatic speech recognition. In a later approach, Qing et al.~\cite{OOD19} propose a two-head deep CNN architectures and maximize the discrepancy between the two classifiers~(MDBC) to detect OOD inputs. It train a two-head CNN consisting of one common feature extractor and two classifiers which have different decision boundaries but can classify ID samples correctly. 


\section{Methods}\label{chapter_6:mm}
In this section, we cover different threat models for different types of adversarial attack. Depending upon scenarios, assumptions, and quality requirements of diagnosis, we then generate AEx to deploy specific attack approaches, followed by adversary training to defend the adversary attacks. 
In this context, robustness is the idea that a model’s prediction is stable to small variations in the input, hopefully because it’s prediction is based on reliable abstractions of the real task that mirror how a human would perform the task. That is, small invisible noise should not flip the prediction from panda to gibbon, small additions to the image should not prevent the model from recognizing a stop sign, and weather conditions or slightly titling the camera should not affect the steering angle of a self-driving car. Hence the explanation is a nice conceptual view, but not very helpful in a practical setting, where we care about whether a prediction is correct. Robustness relates only to the question whether we are near the model’s decision boundary, without knowing anything about the actual decision boundary.

\hspace*{3.5mm} Robustness is an appealing property to study, because it is easy to define well as an invariant over the relation of two inputs (technically a metamorphic relation) without having to rely on specifications and ground truth of what the actual correct prediction is. All local robustness properties more or less follow this form: Given an
input $x$ and a model $f$ of interest, we want the model's prediction to stay the
same for all inputs $x$ ' in the neighborhood of $x$, where the neighborhood is defined by some distance function $\delta$ and some maximum distance $\Delta$ :
$\forall x^{\prime} . \delta\left(x, x^{\prime}\right) \leq \Delta \Rightarrow f(x)=f\left(x^{\prime}\right)$
The way the distance is defined may differ a lot based on the problem.
Typical examples allow for low amounts of noise to all input features (e.g., all pixels), arbitrary changes to few input features (e.g., change any three
pixels), or more complicated transformations (e.g., tilting the picture or
adding "fog"). Whatever the possible transformations or corresponding
distance functions and maximum distance, we always reason about some
neighborhood around the original input.

\hspace*{3.5mm} Note that this definition does not require any information about what the
correct prediction for $f(x)$ or $f\left(x^{\prime}\right)$ is, we simply reason that
the prediction shall stay consistent (whether correct or not) within a
neighborhood.
Robustness is either established as a binary property, i.e., an input for a model is verified as robust or not (usually conservative over approximations,
but also probabilistic judgments with a confidence level have been
proposed), or as some form of relative measure, e.g., the distance to the
nearest adversarial example or the percentage of robust inputs in the
neighborhood. In addition to local robustness, some researchers have also discussed a global robustness property of a model, some form of average robustness for all possible inputs. For example, one could measure for what percent of inputs robustness can be verified or what the average distance is from each input to the nearest adversarial example.

\hspace*{3.5mm} First of all, robustness is difficult to interpret. The only model that is fully robust for all inputs is the trivial model that returns the same prediction for all outputs. For all other models, there is a decision boundary and some inputs will be close to the model’s decision boundary and hence not robust: some parts of the neighborhood of inputs near the decision boundary will be on each side of the decision boundary.
No model is ever fully robust: The black line illustrates a decision boundary between + and - predictions. Some inputs are close to the decision boundary that their neighborhood (circle) extends past the decision boundary.

\subsection{Formulating threat models}
Literature~\cite{yuan2019adversarial}, decomposed the threat models into 4 different aspects: i) adversarial falsification (i.e., in which negative and positive sample are generated with false positive and false negative attacks, respectively), adversary's knowledge~(e.g., depending upon if the deployed model is a white-box or a black-box model), adversarial specificity~(e.g., targeted or non-targeted attacks), and attack frequency~(i.e., one-time or iterative attacks). In a white-box attack scenario, it is assumed that the adversary has sufficient knowledge about the model, including the training data, model architectures, hyper-parameters, numbers of layers, activation functions, and model weights~\cite{yuan2019adversarial}. In order to perform the attack, AEx are generated by calculating model gradients. 

\hspace*{3.5mm} A black-box attack assumes that the adversary has no access to~(or knowledge about) the  model, but knows what the model is for~(e.g., model's confidence score). Although, majority of adversarial attacks are white-box attacks, they can be applied to black-box scenario too, due to the transferability of AEx~\cite{papernot2016transferability}. Targeted attacks misguide a model to a specific class in a in a multiclass classification problem. For example, an adversary can fool the cancer types classifier to predict all the AEx of breast cancer. Targeted attacks usually maximize the probability of targeted adversarial class~\cite{yuan2019adversarial}, while a non-targeted attack does not assign a specific class to the network's output and the class output can be arbitrary. %, except the original one.

\begin{figure}[htp!]
    \centering
    \includegraphics[scale=0.6]{images/aattacks.png}
    \caption[Formulating adversarial threat model]{Two types of attack scenarios: up: minor content moderation across samples with fast gradient sign method~\cite{goodfellow2014explaining}, below: crafting adversarial samples with DeepFool}
    \label{fig:aattacks}
    \vspace{-2mm}
\end{figure}

\iffalse
\begin{figure}[htp!]
    \centering
    \includegraphics[width=0.8\textwidth,height=50mm]{images/robustness_wf.png}
    \caption{Workflow of the methods}
    \label{fig:robustness_workflow}
\end{figure}
\fi 

\hspace*{3.5mm} Since non-targeted attacks are easier to implement compared to targeted attacks, and keeping in mind the scope of this thesis, we performed non-targeted attacks in a black-box scenario only. We apply two types of adversarial attacks on our models: i) attack with AEx generated content moderation attack, ii) attack with AEx generated with DeepFool. %, iii) attack with OOD samples generated with GAN. %The first two attacks falls in the content moderation attack category. 
AEx are expected to be close to the original samples and should be imperceptible to a human, which causes the performance degradation of any ML models compared to that of a human~\cite{goodfellow2014explaining}. Nevertheless, the same adversarial example is often misclassified by a variety of classifiers with different architectures or trained on different subsets of the training data~\cite{yuan2019adversarial,goodfellow2014explaining}. We hypothesize that a black-box model with adversarial robustness capability, will still be able to generate moderately fair and reliable explanations. 

\subsection{Adversarial attacks}
%Before a model is deployed, robustness needs to be improved to improve content detection accuracy under the adversarial attack. 
For each target model, we generate AEx from test samples as outlined in \Cref{fig:aattacks} and use only those that can attack successfully before deploying any counter measure in all of our experiments. 
%\subsubsection{Adding Gaussian noises}
%\subsubsection{Adversarial samples generation}
%The linear view of adversarial examples suggests a fast way of generatingthem.%Since neural networks are too linear to resist linear adversarial perturbations. More nonlinear models such as Sigmoid networks are carefully tuned to spend most of their time in the non-saturating, more linear regime for the same reason. This linear behavior suggests that cheap, analytical perturbations of a linear model should also damage neural networks. 
Two types of attack scenarios introduced in which AEx are generated with content moderation across samples an by crafting AEx with DeepFool.
%, iii) with GAN-based approach. 
As for the first one, FGSM is employed, where the gradient is computed using backpropagation. For a given trained model $f$ and an original input data sample $x$, generating an adversarial example $x^{\prime}$ can be formulated as a box-constrained optimization problem~\cite{yuan2019adversarial}:

\vspace{-6mm}
\begin{align}
    \begin{array}{cl}
        \min _{x^{\prime}} & \left\|x^{\prime}-x\right\| \\
        \text {s.t.} & f\left(x^{\prime}\right)=y^{\prime} \\
        & f(x)=y \\
        & l \neq y^{\prime} \\
        & x^{\prime} \in[0,1],
    \end{array}
\end{align}

\hspace*{3.5mm} where $y$ and $y^{\prime}$ is the predicted label of $x$ and $x^{\prime},\|\cdot\|$ is the distance between two samples. Let $\eta=x^{\prime}-x$ be the perturbation added on $x$ and minimizes the perturbation while misclassifying the prediction with a constraint of input data, ${\theta}$ is the model parameter, ${x}$ is the input, $y$ targets associated with ${x}$ and $J( \theta, x,y)$ is the cost used to train a DNN architecture. The cost function around the current value of ${\theta}$ can be linearize by obtaining an optimal max-norm constrained perturbation as outlined in \cref{eq:fgsm_eta}. To craft AEx, we employ an iterative optimization-based approach called DeepFool, which has higher success rates under the same norm objective in a white-box setting~\cite{yuan2019adversarial}. Since DeepFool provides less perturbation compared to FGSM~\cite{yuan2019adversarial}, we consider AEx generated with FGSM as OOD attack. 

%\subsubsection{Attack with crafted adversarial samples}
%\subsection{OOD attacks}
\iffalse
We generated OOD samples
%~(equal number of samples used to train MCAE classifiers) 
using `Natural GAN'~(NGAN)\footnote{\url{https://github.com/zhengliz/natural-adversary}}~\cite{zhao2017generating}. 
GAN-based AEx are more natural to human. First, we train a Wasserstein GAN model on the PCAt datasets, where the generator $\mathcal{G}$ maps random noise to the input domain. Additionally, we train an ``inverter" $\mathcal{I}$ to map input data to dense inner representations, where adversarial noise is generated by minimizing the distance of the inner representations. AEx were generated using the generator $  x^{\prime}=\mathcal{G}\left(z^{\prime}\right)$, where~\cite{OOD19,zhao2017generating}: 

\vspace{-6mm}
\begin{align}
    \begin{array}{cl}
        \min _{z} & \|z-\mathcal{I}(x)\| \\
        \text {s.t.} & f(\mathcal{G}(z)) \neq f(x)
    \end{array}
\end{align}

\hspace*{3.5mm}Both the generator $\mathcal{G}$ and the inverter $\mathcal{I}$ were built to make adversarial examples natural. Another reason of using NGAN in the black-box attack scenario that it does not require gradients of original neural networks. The GAN-based data generation workflow is shown in \cref{fig:OOD_GAN_generation}. 

\begin{figure*}
	\centering
	\begin{subfigure}{.61\linewidth}
		\centering
		\includegraphics[width=\linewidth,height=60mm]{images/gan_sg.png}
		\caption{OOD sample generation}
        \label{fig:ood_sample-generation}
	\end{subfigure}
	\begin{subfigure}{0.35\linewidth}
		\centering
		\includegraphics[width=\linewidth,height=60mm]{images/ld.png}
		\caption{Layered distribution}
	\end{subfigure}
	\caption{GAN-based OOD sample generation: green \& purple dots represent real and fake samples~\cite{kahng2018gan}} 
	\label{fig:OOD_GAN_generation}
\end{figure*}
\fi 

\subsection{Defenses against attacks}
Literature~\cite{OOD19} introduced two types of countermeasures against adversarial attacks called ``reactive'' and ``proactive'' measures. The former deals with detecting AEx after an ML model is deployed. Examples include adversarial detecting, input reconstruction, and network verification. The latter is about making an ML more robust before an adversary generates and introduce an attack. Examples include network distillation, adversarial (re)training, and classifier robustifying. However, keeping in mind the scope of this thesis, a light weight countermeasures is taken to improve the robustness, as shown in \cref{fig:reactive_measures}. 

\begin{figure}[htp!]
    \centering
    \includegraphics[scale=0.8]{images/reactive_measure.png}
    \caption[Reactive and proactive measures in a nutshell against adversarial attacks]{Workflow of reactive and proactive measures against adversarial sample in inferencing}
    \label{fig:reactive_measures}
    \vspace{-4mm}
\end{figure}

\subsubsection{Proactive measure: adversarial retraining}
%We evaluate the models on adversarial test examples, before employing any defense. 
Given two different gradient-based cancer type~i.e., trained) classifiers $f_1$ and $f_2$~(e.g., $MCAE_{slr}$ and $MCAE_{lrc}$ models trained on the multimodal genomics data based on shared latent representation and latent representation concatenation, respectively), the robustness of each content moderation model is measured by the minimum perturbations required for an input sample to evade detection. For model $f: \mathbb{R}^{D} \rightarrow$ $\{1, \ldots, K\}$~(either $f_1$ or $f_2$), assuming $f(x)=\operatorname{argmax}_{i}\left(Z(x)_{i}\right)$, where $Z(x) \in\mathbb{R}^{K}$ is the final layer output, and $Z(x)_{i}$ is the prediction score w.r.t $i^{th}$ class. Similar to literature~\cite{bhatt2020explainable}, the objective is formulated w.r.t. to find the minimum perturbation as an optimization problem as follows~\cite{bhatt2020explainable}:

\vspace{-6mm}
\begin{align}
    \underset{\boldsymbol{x}}{\operatorname{argmin}}\left\{d\left(\boldsymbol{x}, \boldsymbol{x}_{0}\right)+c {L}(\boldsymbol{f}(\boldsymbol{x}), y)\right\},
\end{align}

\hspace*{3.5mm} where $d(\cdot, \cdot)$ is a distance measure $\ell_{2}$~(i.e., Euclidean distance), ${L}(\cdot)$ is the `cross-entropy loss~(CEL)' function and $c$ is a balancing factor. However, searching for possible AEx is an expensive and non-trivial problem. The projected gradient descent~(PGD) is a commonly used method, which search for the minimum perturbation from the test set of allowable perturbations ${S} \subseteq \mathbb{R}^{D}$ as follows~\cite{bhatt2020explainable}: 

\vspace{-6mm}
\begin{align}
    x^{t+1}=\Pi_{x+\mathcal{S}}\left(x^{t}+\alpha \operatorname{sgn}\left(\nabla_{x} \mathcal{L}\left(f_{\theta^{*}}(x), y\right)\right)\right).
    \label{eq:search_perbu}
\end{align}

\hspace*{3.5mm} The above relation holds until $x^{t}$ is misclassified by the model successfully. Based on the $\ell_{2}$-norm perturbation distance averaged over $n$ test samples: the larger the average perturbation, the more robust the model is, as it takes greater effort for an attacker to evade detection~\cite{bhatt2020explainable,yuan2019adversarial}. 
%The average perturbation required is also widely used as a metric when comparing different candidate models and different versions of a given model. 
Therefore, more robust models have more convincing gradient-based explanations, i.e., the gradient of the output w.r.t the input shows that the model is focusing on relevant portions of an input object~(e.g., image)~\cite{bhatt2020explainable}.

\subsubsection{Reactive measure: input reconstruction}
Now let's assume, we identified AEx with minor content moderation. Therefore, we can also assume that because of different data generation platform~(e.g., human methylation 450K vs. 27K), an adversarial sample is slightly different than a original sample. Based on this assumption, we attempted to still make a meaningful prediction by correcting or reconstructing a clean representation. Nevertheless, AEx can be transformed to corresponding clean input via reconstruction. After the transformation, an adversarial sample will not be able to affect the prediction of a trained model~(at least not that severely). 
To increase the robustness of DNNs, deep contractive autoencoder is proposed by introducing penalty~\cite{meng2017magnet}, were a denoising AE~(DAE) is trained to encode AEx to original ones to remove adversarial perturbations. 

\hspace*{3.5mm} As stated in \cref{chapter:introduction}, noise can be introduced to the input~\cite{min2018survey} to improve the robustness of the representation learning~(RL), followed by the extraction of composing robust features\footnote{We already introduced noise via Gaussian noise layers to make the snapshot models~(refer to \cref{chapter:uni_modality}) learn little variations, which helped both Conv-LSTM and CAE classifiers improve the generalization. }. Hence reconstruction of the AEx is possible by adding Gaussian noise~\cite{OOD19}. First, to simulate the scenario, we added Gaussian noises to features in the GE, miRNA, and CNV data with zero mean and standard deviations of 0-200\% $(k)$ of $i^{\text {th}}$ per feature~(i.e., gene) average $\left(\mu_{i}\right),$ or $N(0, k \mu)$ per feature. We apply PixelCNN proposed in PixelDefend~\cite{song2017pixeldefend} to reconstruct the AEx back to the training distribution, where all pixels are changed along each channel to maximize the probability distribution~\cite{song2017pixeldefend}:

\vspace{-6mm}
\begin{align}
    \begin{array}{cl}
        \max _{x^{\prime}} & {P}_{t}\left(x^{\prime}\right) \\
        \text {s.t.} & \left\|x^{\prime}-x\right\|_{\infty} \leq \epsilon_{\text {defend}},
    \end{array}
\end{align}

\hspace*{3.5mm} where ${P}_{t}$ denotes the training distribution, $\epsilon_{\text {defend}}$ controls the new changes to AEx. For a supplied input~(i.e., perturbed sample) $x^{\prime}$, first we reconstruct a cleaner version $x^c$ using PixelCNN, which we use to classify into a specific cancer type using model $f: {X}^{(m)} \rightarrow {Y}$~(e.g., either $f_1$ or $f_2$ for multimodal inputs and CNN for unimodal input), where predictor $f$ maps instance $x$ from a feature space ${X}^{(m)}$ with $m$ input features to a labels $y$ in a target space ${Y}$, where $f(x)=y$ denotes the decision $y$, predicted by $f$. %Further, we employ PixelDefend for the detection of possible AEx, i.e, if an adversarial example . 

\subsubsection{Reactive measure: out-of-distributions detection}
\noindent The prediction probability of OOD samples tends to be lower than the prediction probability of ID samples~\cite{OOD18}. Therefore, OOD samples are closer to the class boundaries and more likely to be misclassified~(or classified with low confidence) by a classifier trained on ID samples. In our approach, to mitigate OOD issue, the likelihood ratio is calculated for the OOD samples before inferencing to avoid such instance.  
%Inspired by literature~\cite{OOD1}, we trained a variational autoencoder~(VAE) on gene expression dataset for OOD detection by employing a method called likelihood ratio to observe the effects of likelihood score by population level background statistics. %We demonstrate the generality of the proposed method by showing that it significantly improves OOD detection when applied to deep generative models of images. 
%\paragraph{Problem formulation}
We assume that an ID~(sample, label) pair, $\left\{\mathrm{x}_{\text {in}}, y_{in}\right\}$ drawn from a set of labeled ID samples $\left\{X_{in}, Y_{in}\right\}$ and $\mathbf{x}_{\mathbf{ul}}$ is drawn from unlabeled samples, and $\mathrm{x}_\text{in} \in \left\{\mathrm{x}_{\text {ge}}, \mathrm{x}_{\text {mr}}, \mathrm{x}_{\text {cnv}}\right\}$, when all the individual modalities are considered. The ID sample $\left\{\mathbf{x}_{\text {in}}, y_{in}\right\}$ can be classified into $K$ classes, i.e., $y_{in} \in K$, and $\mathbf{x}_{\mathbf{ul}}$ can be either an ID or an OOD samples, where $\exists\left\{\mathbf{x}_{\text {ul}}, y_{ul}\right\}, y_{ul} \notin K$. 

\begin{figure}
    \centering
    \includegraphics[scale=0.6]{images/ood_ex_1.png}
    \caption[OOD sample detection w.r.t discrepancy between two different  classifiers]{Detection of OOD samples by the discrepancy between two classifiers based on~\cite{OOD19}, where $MCAE_{slr}$ and $MCAE_{lrc}$ represent model $f_1$ and $f_2$, respectively}
    \label{fig:ood_detection_example}
    \vspace{-4mm}
\end{figure}

\hspace*{3.5mm} Since the goal is to distinguish whether a sample $\mathbf{x}_{\mathbf{ul}}$ is from ID or OOD, we train the network to predict different softmax class probabilities for ID samples and OOD samples by utilizing $\mathbf{x}_{\mathbf{u}1}$ during the unsupervised training as literature~\cite{OOD19}. From a given ID dataset ${D}$ of $(\boldsymbol{x}, y)$ pairs sampled from the distribution $p^{*}(\boldsymbol{x}, y)$ where $x$ is the extracted feature vector or raw input and $y \in {Y}:=\{1, \ldots, k, \ldots, K\}$ is the label assigning membership to one of $K$ ID classes. In the corresponding profiles~(i.e., GE, miRNA, or CNV), inputs are numeric or discrete, i.e. $x_{d} \in \mathbb{R}$. In general, OOD inputs are sampled $(\boldsymbol{x}, y)$ generated from an underlying distribution other than $p^{*}(\boldsymbol{x}, y)$. Similar to literature~\cite{OOD1}, we then consider an input $(x, y)$ to be OOD if $y \notin {Y}$, i.e., the class $y$ does not belong to any of $K$ ID classes. This turns out the OOD detection is a task to accurately detect if an input $x$ is OOD or not~\cite{OOD1,OOD2,OOD3}.

%\paragraph{Network constructions}
\hspace*{3.5mm} Similar to literature~\cite{yu2019unsupervised}, we construct a two-head neural network architectures consisting of a feature extractor, $E,$ which takes inputs $\mathrm{x}_{\text {in }}$ or $\mathrm{x}_{\mathrm{ul}}$, and two classifier $f_{1}$ and $f_{2}$, where each classifier takes features from $E$ and predict into one of $K$ classes. Classifiers $f_{1}$ and $f_{2}$ output a $K$ dimensional vector of logits. The class probabilities is then calculated using softmax activation, where $p_{1}(\mathbf{y} | \mathbf{x})$ and $p_{2}(\mathbf{y} | \mathbf{x})$ are $K$ dimensional softmax class probabilities for input $x$ produced with classifier $f_{1}$ and $f_{2}$, respectively. In our case, we assume each sample consisting of 3 different modalities, i.e., gene expression, copy numbers, and miRNA expression, s.t., we create latent representation of each input modality as shown in \cref{fig:ood_network}. 

\begin{sidewaysfigure*}
    \centering
    \includegraphics[scale=0.9]{images/OOD_1.png}
    \caption[Supervised pre-training and unsupervised fine-tuning steps for OOD detection]{Supervised pre-training and unsupervised fine-tuning steps for OOD detection method which we extended for multimodal scenario is based on~\cite{yu2019unsupervised}. The network consists of an extractor $E$ and two classifiers $f_{1}$ and $f_{2}$. \textbf{Step-A}: supervised training to classify ID samples correctly. \textbf{Step-B}: $f_{1}$ and $f_{2}$ learn to maximize the discrepancy in an unsupervised manner to detect OOD samples}
    \label{fig:ood_network}
    \vspace{-2mm}
\end{sidewaysfigure*}

\hspace*{3.5mm} If two different classifiers~(i.e. $f_{1}$ and $f_{2}$ in our case) are initialized with random initial parameters and subsequently trained on ID samples supervised way, they are tend to show slightly different characteristics. Nevertheless, most OOD samples have larger discrepancy than ID samples, two different classifiers classify OOD samples differently~\cite{OOD19}. 
%Based on the characteristic of OOD samples,  
We measure the disagreement between the classifiers and train the network to maximize it. Similar to literature~\cite{OOD19}, it is expected that the network will push OOD samples outside the manifold of ID samples. Consequently, OOD samples and ID samples can be separated according to the discrepancy between the two classifiers’ outputs as shown in \cref{fig:ood_detection_example}, where discrepancy loss is computed as follows to measure the divergence between the two softmax class probabilities for an input~\cite{OOD19}: 

\vspace{-6mm}
\begin{align}
    d\left(p_{1}(\mathbf{y} | \mathbf{x}), p_{2}(\mathbf{y} | \mathbf{x})\right)=H\left(p_{1}(\mathbf{y} | \mathbf{x})\right)-H\left(p_{2}(\mathbf{y} | \mathbf{x})\right),
    \label{eq:dis_loss_1}
\end{align}

\hspace*{3.5mm} where $H(\cdot)$ is the entropy over the softmax distribution. Again similar to literature~\cite{yu2019unsupervised}, the whole training consists of supervised pre-training and unsupervised fine-tuning. During the pre-training, we train the network to learn discriminative features before classifying the ID samples correctly in a supervised way. Since this is a supervised way to predict the labels, we optimize the following CEL~\cite{yu2019unsupervised}:

\vspace{-6mm}
\begin{align}
    {L}_{sup}=-\frac{1}{\left|X_{i n}\right|} \sum_{\mathbf{x}_{\mathrm{in}} \in X_{i n}} \sum_{i=1}^{2} \log \left(p_{i}\left(y_{i n} | \mathbf{x}_{\mathrm{in}}\right)\right).
\end{align}

\hspace*{3.5mm} Once the network converges, we perform unsupervised fine-tuning to increase the discrepancy in an unsupervised manner in order to make the network detect the OOD samples that do not have the support of the ID samples. During the supervised learning, we keep training the network to classify the labeled ID samples by minimizing the CEL to maintain the manifold of ID samples. To optimize both supervised and unsupervised losses, we first combine them and optimize jointly similar to literature~\cite{yu2019unsupervised}: 

\vspace{-4mm}
\begin{align}
    \begin{array}{c}
        {L}={L}_{sup}+{L}_{unsup} \\ 
        {L}_{sup}=-\frac{1}{\left|X_{i n}\right|} \sum_{\mathbf{x}_{\mathrm{in}} \in X_{in}} \sum_{i=1}^{2} \log \left(p_{i}\left(y_{i n} | \mathbf{x}_{\mathrm{in}}\right)\right) \\ 
        {L}_{unsup}=\max \left(m-\frac{\sum_{\mathbf{x}_{\mathbf{u} 1} \in X_{ul}} d\left(p_{1}\left(\mathbf{y} | \mathbf{x}_{\mathbf{ul}}\right), p_{2}\left(\mathbf{y} | \mathbf{x}_{\mathbf{ul}}\right)\right)} {\left|X_{ul}\right|},0\right),
    \end{array}
    \label{eq:ood_join_loss}
\end{align}

\hspace*{3.5mm} where, $m$ is the margin used to prevent overfitting, i.e., if the average discrepancy of unlabeled samples is greater than the margin m, the unsupervised loss will equal its minimum value zero~\cite{yu2019unsupervised}. Since the network is already trained to maximize the discrepancy loss, which means the the entropy of $f_1$’s output is tend to be maximized and the entropy of $f_2$’s output to get minimized. Consequently, $f_2$ is expected to predict high probability of one class. In that case, OOD samples will be outside the support of the ID samples making the discrepancy between the outputs of $f_{1}$ and $f_{2}$ for OOD samples to be larger. During the inference time, in order to distinguish between ID and OOD samples, similar to literature~\cite{yu2019unsupervised}, the discrepancy is computed w.r.t, the $L_1$ distance between $f_{1}$ and $f_{2}$ classifier's outputs. When the distance is above a detection threshold $\delta$, we consider the sample as an OOD sample denoted as follows~\cite{yu2019unsupervised}: 

\vspace{-6mm}
\begin{align}
    \sum_{i=1}^{K}\left|p_{1}\left(y_{i} | \mathbf{x}\right)-p_{2}\left(y_{i} | \mathbf{x}\right)\right|>\delta, 
\end{align}

\hspace*{3.5mm} where $x$ could be a unimodal or multimodal sample. Subsequently, we stop making the inferencing for an OOD samples, given that making a wrong prediction is far better than a garbage prediction. 

\section{Experiments}\label{chapter_6:results} 
In this section, we discuss the evaluation results of content moderation attack and OOD detection, both quantitatively and qualitatively. %Besides, a comparative analysis with state-of-the-art approaches is provided. 
More specifically, several experiments were carried out to improve the adversarial robustness of the models, with the following objectives:

\begin{enumerate}[noitemsep]
    \item Can we detect if a supplied input is an adversarial sample~(i.e., ID or OOD sample)? 
    \item Can the model reconstruct, from noise input, a cleaner version of the sample? 
    \item Which neural network architecture is more robust against different adversarial attacks?  
    \item Considering the attacks scenarios, what types of counter measures is useful for genomic data?
\end{enumerate}

\hspace*{3.5mm} Results of each observation are analysed, both quantitative and qualitatively, in the following section. 

\subsection{Experiment setup}
%All the programs were written in Python\footnote{\url{https://github.com/rezacsedu/XAI_Cancer_Prediction}}. The software stack consists of scikit-learn and Keras with TensorFlow backend. The network was trained on an Nvidia GTX 1080i GPU with CUDA and cuDNN enabled to make the overall pipeline faster. 
%Further, since an appropriate selection of hyperparameters can have a huge impact on the performance of a deep architecture, we perform the hyperparameter optimization through a random search and 5-fold cross-validation tests. In each of 5 runs, 70\% of the data is used for the training, 30\% for the evaluation, while 10\% of the training set is used for validation of the networks to optimize the cross-entropy loss based on the best learning rate, batch size, number of filters, kernel size, and dropout/Gaussian noise probability. 
%\subsubsection{Setup for content moderation detection}
%\subsubsection{Setup for OOD detection}
For the pre-training, we follow steps similar to literature~\cite{OOD19}, but using Adam optimizer to train both MCAE classifiers for 200 epochs. The learning rate~(LR) started at 0.1 and dropped by a factor of 10 at 50\% and 75\% of the training progress, respectively. During the fine-tune, we iterate each fit for 50 epochs, by setting margin m = 1:2 to detect OOD samples. 
%Subsequently, we select the best classifier using WeightWatcher. %Results based on hyperparameters produced through random search and 5-fold cross-validation are reported and discussed with a comparative analysis with AUROC, AUPR, FPR, TPR, FNR, and DE. 
To assess the robustness of our models, we compute the empirical robustness metric~(ERM) of a classifier object over the test set for a given adversarial crafting method attack as suggested in literature~\cite{moosavi2016deepfool}. ERM is equivalent to computing the minimal perturbation that the attacker must introduce for a successful attack and is measured as follows~\cite{moosavi2016deepfool}:

\vspace{-4mm}
\begin{align}
    \hat{\rho}_{\mathrm{adv}}^{\infty}(f)=\frac{1}{|D|} \sum_{\boldsymbol{x} \in D} \frac{\|\hat{\boldsymbol{r}}(\boldsymbol{x})\|
    _{\infty}}{\|\boldsymbol{x}\|_{\infty}},
\end{align}

\hspace*{3.5mm} where $\hat{\boldsymbol{r}}(\boldsymbol{x})$ is computed respectively using DeepFool~(with $p=\infty$) and the FGSM. The higher the ERM value, the higher the a classifier is. Besides, we compute the CLEVER~(short for \textbf{C}ross \textbf{L}ipschitz \textbf{E}xtreme \textbf{V}alue for n\textbf{E}twork \textbf{R}obustness) score for an untargeted attack based on literature~\cite{weng2018evaluating}. 
The CLEVER score is attack-agnostic and computationally feasible for large neural networks, given the fact that CLEVER is aligned with the robustness indication measured by the $\ell_{2}$ and $\ell_{\infty}$ norms \footnote{The \textbf{$\ell_{1}$ norm}~(also known as Manhattan Distance or Taxicab norm) gives the magnitudes of the vectors in a space. The \textbf{$\ell_{2}$ norm}~(also known as the Euclidean norm) gives the shortest distance to go from one point to another. The \textbf{$\ell_{\infty}$ norm} gives the largest magnitude among each element of a vector, i.e., only the largest element has the effect.} of adversarial examples from powerful attacks. 
%Besides, it is defended networks using defensive distillation or bounded ReLU indeed achieve better CLEVER scores~\cite{weng2018evaluating}. 
Besides, false positive rates~(FPR), detection errors~(DE), Area Under the Receiver Operating Characteristic curve~(AUROC), Area Under the Precision-Recall~(AUPR), are computed to assess robustness of the models against OOD attack, as~\cite{OOD19,OOD18}, with the following settings: 

\vspace{-2mm}
\begin{itemize}[noitemsep]
    \item FPR at 95\% TPR shows the FPR at 95\% true positive rate~(TPR)
    \item DE measures the minimum misclassification probability calculated as the minimum average of FPR and false negative rate~(FNR) over all possible score thresholds.
    \item AUROC is calculated by the AUPR against TPR curve.
    \item AUPR In is calculated by the area under the precision against the recall curve, where ID samples are treated as positive.
    \item AUPR Out is similar to AUPR In, except for OOD samples are treated as positive. 
\end{itemize}
%\vspace{-2mm}
%\subsection{Content moderation robustness}

\iffalse
\begin{sidewaystable*}[htp!]
	\centering
	\caption{Results of ablation study of two different classifiers covering detection error, ID/OOD discrepancy losses across different sample size} 
	\scriptsize{
	\begin{tabular}{lllllllll}
		& & \multicolumn{3}{c}{\bfseries{$f_1$}} && \multicolumn{3}{c}{\bfseries{$f_2$}} \\	
		\cmidrule{3-5}\cmidrule{7-9}   
		\textbf{Modality~(ID+OOD)}&\textbf{\#Samples}& \textbf{Detection error} & \textbf{ID discrepancy loss}& \textbf{OOD discrepancy loss} 
		&& \textbf{Detection error} & \textbf{ID discrepancy loss}& \textbf{OOD discrepancy loss}\\
		\hline
		\multirow{8}{*}{{GE}} & 100 & 0.75 & 0.72 & 0.87 && 0.70 & 0.70 & 0.70\\
		& 300 & 0.88 & 0.88 & 0.88  && 0.75 & 0.74 & 0.74 \\
    	& 600 & 0.88 & 0.88 & 0.88 && \textbf{0.77} & \textbf{0.77} & \textbf{0.76} \\
		& 900 & 0.83 & 0.83 & 0.83 && 0.65 & 0.64 & 0.64\\
		& 1200 & 0.83 & 0.83 & 0.83 && 0.68 & 0.68 & 0.68\\
		& 1500 & 0.82 & 0.82 & 0.82 && 0.64 & 0.65 & 0.64\\
		& 1800 & 0.82 & 0.81 & 0.81 && 0.68 & 0.66 & 0.67\\
		\hline
		\multirow{8}{*}{{MR}} & 100 & 0.67 & 0.66 & 0.66 && 0.58 & 0.60 & 0.59\\
		& 300 & 0.69 & 0.67 & 0.68 && 0.59 & 0.60 & 0.59\\
		& 600 & 0.68 & 0.66 & 0.67 && 0.61 & 0.63 & 0.62\\
		& 900 & 0.64 & 0.62 & 0.63 && 0.55 & 0.58 & 0.56\\
		& 1200 & 0.66 & 0.64 & 0.65 && 0.58 & 0.59 & 0.58\\
		& 1500 & 0.64 & 0.62 & 0.63 && 0.57 & 0.59 & 0.58\\
		& 1800 & 0.64 & 0.63 & 0.63 && 0.52 & 0.57 & 0.54\\
		\hline
		\multirow{8}{*}{{CNV}} & 100 & 0.87 & 0.87 & 0.87 && 0.70 & 0.70 & 0.70\\
		& 300 & 0.88 & 0.88 & 0.88  && 0.75 & 0.74 & 0.74 \\
    	& 600 & 0.88 & 0.88 & 0.88 && \textbf{0.77} & \textbf{0.77} & \textbf{0.76} \\
		& 900 & 0.83 & 0.83 & 0.83 && 0.65 & 0.64 & 0.64\\
		& 1200 & 0.83 & 0.83 & 0.83 && 0.68 & 0.68 & 0.68\\
		& 1500 & 0.82 & 0.82 & 0.82 && 0.64 & 0.65 & 0.64\\
		& 1800 & 0.82 & 0.81 & 0.81 && 0.68 & 0.66 & 0.67\\
		\hline
		\multirow{8}{*}{{GE+CNV}} & 100 & 0.67 & 0.66 & 0.66 && 0.58 & 0.60 & 0.59\\
		& 300 & 0.69 & 0.67 & 0.68 && 0.59 & 0.60 & 0.59\\
		& 600 & 0.68 & 0.66 & 0.67 && 0.61 & 0.63 & 0.62\\
		& 900 & 0.64 & 0.62 & 0.63 && 0.55 & 0.58 & 0.56\\
		& 1200 & 0.66 & 0.64 & 0.65 && 0.58 & 0.59 & 0.58\\
		& 1500 & 0.64 & 0.62 & 0.63 && 0.57 & 0.59 & 0.58\\
		& 1800 & 0.64 & 0.63 & 0.63 && 0.52 & 0.57 & 0.54\\
		\hline
		\multirow{8}{*}{{GE+MR}} & 100 & 0.87 & 0.87 & 0.87 && 0.70 & 0.70 & 0.70\\
		& 300 & 0.88 & 0.88 & 0.88  && 0.75 & 0.74 & 0.74 \\
    	& 600 & 0.88 & 0.88 & 0.88 && \textbf{0.77} & \textbf{0.77} & \textbf{0.76} \\
		& 900 & 0.83 & 0.83 & 0.83 && 0.65 & 0.64 & 0.64\\
		& 1200 & 0.83 & 0.83 & 0.83 && 0.68 & 0.68 & 0.68\\
		& 1500 & 0.82 & 0.82 & 0.82 && 0.64 & 0.65 & 0.64\\
		& 1800 & 0.82 & 0.81 & 0.81 && 0.68 & 0.66 & 0.67\\
		\hline
		\multirow{8}{*}{{CNV+MR}} & 100 & 0.67 & 0.66 & 0.66 && 0.58 & 0.60 & 0.59\\
		& 300 & 0.69 & 0.67 & 0.68 && 0.59 & 0.60 & 0.59\\
		& 600 & 0.68 & 0.66 & 0.67 && 0.61 & 0.63 & 0.62\\
		& 900 & 0.64 & 0.62 & 0.63 && 0.55 & 0.58 & 0.56\\
		& 1200 & 0.66 & 0.64 & 0.65 && 0.58 & 0.59 & 0.58\\
		& 1500 & 0.64 & 0.62 & 0.63 && 0.57 & 0.59 & 0.58\\
		& 1800 & 0.64 & 0.63 & 0.63 && 0.52 & 0.57 & 0.54\\
		\hline
		\multirow{8}{*}{{GE+MR+CNV}} & 100 & 0.87 & 0.87 & 0.87 && 0.70 & 0.70 & 0.70\\
		& 300 & 0.88 & 0.88 & 0.88  && 0.75 & 0.74 & 0.74 \\
    	& 600 & 0.88 & 0.88 & 0.88 && \textbf{0.77} & \textbf{0.77} & \textbf{0.76} \\
		& 900 & 0.83 & 0.83 & 0.83 && 0.65 & 0.64 & 0.64\\
		& 1200 & 0.83 & 0.83 & 0.83 && 0.68 & 0.68 & 0.68\\
		& 1500 & 0.82 & 0.82 & 0.82 && 0.64 & 0.65 & 0.64\\
		& 1800 & 0.82 & 0.81 & 0.81 && 0.68 & 0.66 & 0.67\\
		\hline
	\end{tabular}}
	\label{Table:OOD_result_1}
\end{sidewaystable*}
\fi 

\hspace*{3.5mm} CLEVER and ERM metrics were computed using the Adversarial Robustness Toolbox~(aka. ART)\footnote{\url{https://github.com/Trusted-AI/adversarial-robustness-toolbox}}. Besides, scikit-learn was used for computing FPR, TPR, FNR, AUPR~(both IN and Out), and AUROC. 

\subsection{Robust classifier's performance on the adversarial samples}
With the above experimental setting, first, we generate some adversarial samples to attack both of our $f_1$ and $f_2$ models. Subsequently, we retrain both models with adversarial samples generated with DeepFool to make them robust. In both cases, the retrained models are much more robust than the originally trained model as shown in \cref{fig:normal_vs_robust_models}, where $\epsilon \in\{0.01, 0.02, 0.03, 0.04, 0.05, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9\}$ to achieve the smallest $\ell_{\infty}$ distortion for individual sample. However, the retrained model based on latent representation concatenation shows more robustness then the shared latent representation. 
\begin{figure*}[h]
	\centering
	\begin{subfigure}{.48\linewidth}
		\centering
		\includegraphics[scale=1.0]{images/adversarial_training_1.png}
		\caption{$MCAE_{lrc}$}
        \label{fig:normal_vs_robust_f1}
	\end{subfigure}
	\begin{subfigure}{0.48\linewidth}
		\centering
		\includegraphics[scale=1.0]{images/adversarial_training_2.png}
		\caption{$MCAE_{slr}$}
        \label{fig:normal_vs_robust_f2}
	\end{subfigure}
	\caption{Performance comparison: original vs.  robust classifier w.r.t  different \textit{eps} values} 
	\label{fig:normal_vs_robust_models}
\end{figure*}

\begin{table}[h]
    \centering
    \caption{Values of $\hat{\rho}_{\text {adv }}^{\infty}$ for two different networks based on DeepFool and FGSM}
    \begin{tabular}{l|c|c}
        \hline \textbf{Classifier} & \textbf{DeepFool} & \textbf{Fast gradient sign method} \\ \hline $f_1$ & 0.29 & 0.09 \\ \hline 
        $f_2$ & 0.21 & 0.05 \\
        \hline
    \end{tabular}
    \label{tab:robustness_result}
\end{table}

\hspace*{3.5mm} Classifier $f_1$ found robust even when $\epsilon$ is increased upto 0.4, giving upto 85\% accuracy. In contrast, $f_2$ shows fragile performance, giving drastic reduction of the accuracy by 25\%. \Cref{tab:robustness_result} reports the $\ell_{\infty}$ robustness to adversarial perturbations for 4 different models based on DeepFool and FGSM with $90\%$ of misclassification. \Cref{tab:robustness_result} shows ERM scores for both classifiers. Since a higher ERM value indicates a more robust classifier, $f_1$ found to be more robust against both DeepFool and FGSM attacks. 

\begin{table}
    \centering
    \caption{Comparison between average untargeted CLEVER score and distortion}
    \begin{tabular}{l|l|l|l|l|}
    \cline {2-5} \multicolumn{1}{c|} {} & \multicolumn{2}{c|} {\text {DeepFool}} & \multicolumn{2}{c|} {\text {FGSM}} \\
    \hline Classifier & $\ell_{2}$ & $\ell_{\infty}$ & $\ell_{2}$ & $\ell_{\infty}$ \\
    \hline $f_1$ & 89.25 & 87.54 & 89.25 & 87.54 \\
    \hline $f_2$ & 83.21 & 82.32 & 86.16 & 85.37 \\
    \hline
    \end{tabular}
    \label{tab:my_label}
\end{table}

\iffalse
\begin{table*}[htp!]
	\centering
	\caption{Results of distinguishing ID and OOD (full) test sets of two different classifiers covering detection error, FPR, AUROC, AUPR In~(AUPRI), and AUPR Out~(AUPRO)} 
	\scriptsize{
	\vspace{-4mm}
	\begin{tabular}{llllllllllll}
		&\multicolumn{5}{c}{\bfseries{$MCAE_{lrc}$}} && \multicolumn{5}{c}{\bfseries{$MCAE_{slr}$}} \\	
		\cmidrule{2-6}\cmidrule{8-12}   
		\textbf{Attack type}&\textbf{$\ell_{2}$} & \textbf{$\ell_{\infty}$} & \textbf{AUROC$\uparrow$} & \textbf{AUPRI$\uparrow$} & \textbf{AUPRO$\uparrow$}
		&& \textbf{FPR$\downarrow$} & \textbf{DE$\downarrow$} & \textbf{AUROC$\uparrow$} & \textbf{AUPRI$\uparrow$} & \textbf{AUPRO$\uparrow$}\\
		\hline
		\multirow{1}{*}{{DeepFool}} & 100 & 0.87 & 0.87 & 0.87 & 0.87 && 0.70 & 0.70 & 0.70 & 0.83 & 0.87\\
		\hline
		\multirow{1}{*}{{FGSM}} & 100 & 0.67 & 0.66 & 0.66 & 0.87 && 0.58 & 0.60 & 0.59 & 0.60 & 0.59\\
		\hline
	\end{tabular}}
	\vspace{-2mm}
	\label{Table:OOD_result_2}
\end{table*}
\fi 

\hspace*{3.5mm} On the other hand, in \cref{tab:my_label}, we provide the comparison between the average untargeted CLEVER scores and distortion based on the FGSM and DeepFool untargeted attacks. As expected, CLEVER is smaller than the distortions of adversarial samples in most cases. Since CLEVER is independent of attack algorithms, the reported CLEVER scores roughly indicate the distortion of the best possible attack w.r.t a specific $\ell_{p}$ distortion. CLEVER scores can be considered as a security checkpoint for unseen attacks. Since we observe substantial gaps in distortion between the CLEVER score and the considered attack algorithms, it suggest the existence of a more effective attack that can close the gap~\cite{weng2018evaluating}.

\subsection{OOD detection performance}
Our approach based on~\cite{OOD19}, can distinguish between in- and OOD samples moderately. As shown in \cref{fig:max_soft_score}, there is moderately less overlap between OOD samples and ID samples on the dataset. The relationship between the discrepancy loss and the DE on the test samples are shown in \cref{Table:OOD_result_2}. As observed the discrepancy loss of ID samples is smaller than OOD samples in most of the settings. It can also be noticed that the DE is lower when the difference between the discrepancy loss of ID and OOD samples is larger, which means ID and OOD samples can be separated by the divergence between the two classifiers’ outputs. As demonstrated, more test sample help decrease both DE, ID/OOD discrepancy losses, upto a certain point. For example, when we evaluate the models on 1,800 samples, all the 3 metrics increased, which is probably because of more discrepancy loss. 

\iffalse
\begin{figure}
    \centering
    \includegraphics[width=\textwidth,height=60mm]{images/dis_loss.png}
    \caption{Histogram of the discrepancy loss between two classifiers trained on ID samples}
    \label{fig:hist_discre_loss}
\end{figure}
\fi 

%, indicating that our method separates ID and OOD samples very well. Another merit of our method is that we can use a simple
%threshold 1:0 to separate ID and OOD samples as shown in
%Fig. 5a.

\iffalse
\begin{figure}
    \centering
    \includegraphics[width=0.5\textwidth,height=60mm]{images/ood_detection.png}
    \caption{Histogram of ID and OOD detection scores}
    \label{fig:ood_detection}
\end{figure}
\fi 

\begin{sidewaystable*}[htp!]
	\centering
	\caption[Results of distinguishing ID and OOD of different classifiers]{Results of distinguishing ID and OOD of two different classifiers covering detection error, FPR, AUROC, AUPR In~(AUPRI), and AUPR Out~(AUPRO)} 
	%\scriptsize{
	\vspace{-4mm}
	\begin{tabular}{llllllllllll}
		&\multicolumn{5}{c}{\bfseries{$f_1$}} && \multicolumn{5}{c}{\bfseries{$f_2$}} \\	
		\cmidrule{2-6}\cmidrule{8-12}   
		\textbf{Modality}&\textbf{FPR$\downarrow$} & \textbf{DE$\downarrow$} & \textbf{AUROC$\uparrow$} & \textbf{AUPRI$\uparrow$} & \textbf{AUPRO$\uparrow$}
		&& \textbf{FPR$\downarrow$} & \textbf{DE$\downarrow$} & \textbf{AUROC$\uparrow$} & \textbf{AUPRI$\uparrow$} & \textbf{AUPRO$\uparrow$}\\
		\hline
		\multirow{1}{*}{{GE}} & 100 & 0.87 & 0.87 & 0.87 & 0.87 && 0.70 & 0.70 & 0.70 & 0.83 & 0.87\\
		\hline
		\multirow{1}{*}{{miRNA}} & 100 & 0.67 & 0.66 & 0.66 & 0.87 && 0.58 & 0.60 & 0.59 & 0.60 & 0.59\\
		\hline
		\multirow{1}{*}{{CNV}} & 100 & 0.87 & 0.87 & 0.87 & 0.66 && 0.70 & 0.70 & 0.70 & 0.60 & 0.59\\
		\hline
		\multirow{1}{*}{{GE+miRNA}} & 100 & 0.67 & 0.66 & 0.66 & 0.66 && 0.58 & 0.60 & 0.59 & 0.60 & 0.59\\
		\hline
		\multirow{1}{*}{{GE+CNV}} & 100 & 0.87 & 0.87 & 0.87 & 0.66 && 0.70 & 0.70 & 0.70 & 0.60 & 0.59\\
		\hline
		\multirow{1}{*}{{miRNA+CNV}} & 100 & 0.67 & 0.66 & 0.66 & 0.66 && 0.58 & 0.60 & 0.59 & 0.60 & 0.59\\
		\hline
		\multirow{1}{*}{{GE+miRNA+CNV}} & 100 & 0.87 & 0.87 & 0.87 & 0.66 && 0.70 & 0.70 & 0.70 & 0.60 & 0.59\\
		\hline
	\end{tabular} %}
	\vspace{-2mm}
	\label{Table:OOD_result_2}
\end{sidewaystable*}

\hspace*{3.5mm} Besides, we observe moderately high disagreement, w.r.t between two classifiers' outputs based on unlabeled ID and OOD samples after training the network on labeled ID samples in a supervised way. The histogram of the classifier's maximum softmax scores in \cref{fig:max_soft_scores_and_de_scores} shows that the distribution of the score moderately different w.r.t whether a sample is ID or OOD after the fine-tuning: $p_{1k}$ and $p_{2k}$ denote the probability distribution for class $k$. The discrepancy loss makes OOD sample's maximum $p_{1k}$ close to $1/K$ and maximum $p_{2k}$ close to 1. On the other hand, ID samples' $p_{1k}$ mostly similar to $p_{2k}$, which is based on the support outlined in \cref{eq:ood_join_loss}.

\iffalse
\begin{figure}
    \centering
    \includegraphics[width=\textwidth,height=60mm]{images/hist_max_softmax.png}
    \caption{Histogram of the two classifier's maximum softmax scores after the fine-tuning}
    \label{fig:hist_softmax_scores}
\end{figure}
\fi 

\begin{figure*}[h]
	\centering
	\begin{subfigure}{.48\linewidth}
		\centering
		\includegraphics[width=\linewidth,height=50mm]{images/ood_detection.png}
		\caption{Histogram of maximum softmax scores}
        \label{fig:max_soft_score}
	\end{subfigure}
	\begin{subfigure}{0.48\linewidth}
		\centering
		\includegraphics[width=\linewidth,height=50mm]{images/hist_max_softmax.png}
		\caption{Histogram of ID and OOD detection scores}
		\label{fig:id_ood_de_scores}
	\end{subfigure}
	\caption{Histogram of the maximum softmax and ID/OOD detection scores of the classifiers} 
	\label{fig:max_soft_scores_and_de_scores}
\end{figure*}

\hspace*{3.5mm} As seen, the discrepancy loss of ID samples is smaller than OOD samples in all settings. The DE is lower when the difference between the discrepancy loss of ID and OOD samples is larger, which means ID and OOD samples can be separated by the divergence between the two classifiers' output. Since this method needs to fine-tune the classifiers in order to detect the OOD samples, the decision boundary changed slightly. Eventually, we see a significant drop of classification accuracy~(by 8\%) compared to original accuracy we observed in \cref{chapter:xai}. We can observe that GE and miRNA are more sensitive to making changes, which reflect in the GE, miRNA, GE+miRNA modalities too. 

%This problem could be solved by using the original classifier to classify ID samples with some runtime increase and it is still much more acceptable than ELOC [26] using an ensemble of five models which needs more runtime and computing resources.

%\section{Discussion} \label{chapter_6:discussion}

\section{Chapter Summary} \label{chapter_6:conclusion}
In this chapter, discussed how to improve the adversarial robustness of an explainable model. We apply different types of adversarial attacks on our model by generating AEx with content moderation attack by introducing noise, generating AEx as OOD. Then we impose both proactive and reactive counter measurements before assessing the robustness against each scenario. 
For detecting OOD, we trained two different classifiers to detect OOD samples that are far from the support of the ID samples. However, considering the scope of this thesis, we try to limit the discussion within simple methods. In future, we intend to focus more employ not sophisticated counter-measurements. To summarize, this chapter disseminate the following findings, through several experiments:

\begin{enumerate}[noitemsep]
    \item \textbf{Can we detect if a supplied input is an adversarial sample?} The approach used in this chapter can moderately identify if a supplied sample is an adversarial sample Overall, both ID and OOD samples can be identified. 
    
    \item \textbf{Can the model reconstruct, from noise input, a cleaner version of the sample?} Adding Gaussian noises during retraining already made both classifier robust against minor perturbations. Nevertheless, PixelCNN is found to be effective at reconstructing, from perturbations samples, the cleaner versions moderately. However, this approach is found to have very limited effectiveness against OOD attacks. 
    
    \item \textbf{Which neural network architecture is more robust against different adversarial attacks?} The classifier $MCAE_{lrc}$ classifier found to be more robust compared to $MCAE_{slr}$. Probably, the reason is that the former classifier still can learn more abstract features from the reconstructed inputs from PixelCNN, which help in the classification as well.  
    
    \item \textbf{Considering the attacks scenarios, what types of counter measures is useful for genomic data?} As DeepFool provides less perturbations, it is found to be moderately strong attack. However, FGSM is found to be too strong attacks, as it provides higher perturbations. Subsequently, both classifiers perform moderately well against DeepFool attack, albeit $MCAE_{lrc}$ classifier found to be more robust against both attacks types compared to $MCAE_{slr}$. Besides, GE and miRNA are more sensitive to making changes, which reflect in the GE, miRNA, GE+miRNA modalities too. 
\end{enumerate}

%Our method does not require labeled OOD samples to train the neural network. 
%We extensively evaluated our method not only on OOD detection benchmarks, but also on real-world simulation datasets. Our method significantly outperformed the current state-of-the-art methods on different DNN architectures across various in and out-of-distribution dataset pairs. 

\hspace*{3.5mm} In the previous chapter, we have seen that explaining the predictions with plots and charts are good for exploration and discovery, but interpreting them for the first time may be very difficult, especially for the non-domain experts/users. This is also applicable for the patients. Therefore, in the next chapter, we provide more human-understandable and model-agnostic explanations based on IF-THEN rules. Using anchors, we will see how to explain individual predictions of our black-box models by finding a decision rule that ``Rule-matrix" and ``Bayesian Rule List" the prediction sufficiently. 
%In \cref{chapter:nsr}, we will see how to combine model predictions, interpretation, and reasoning to provide more fair rules. 

