\chapter{Conclusion}\label{chapter:end}
\textit{``It is always important to know when something has reached its end. Closing circles, shutting doors, finishing chapters, it doesn't matter what we call it; what matters is to leave in the past those moments in life that are over.''} - Paulo Coelho

\section{Overall Summary}
Now that we came to an end of our explainable journey. Let's see a quick recap of each chapter: we covered the foundations and concepts in \cref{chapter:preli} that we used in the subsequent chapters. In \cref{chapter:uni_modality}, we developed two predictive models: Conv-LSTM and CAE classifier based on single modality to find the association between CNV data and cancer. Then we perform the classification of 12 different cancer types. Nevertheless, the findings motivated us to use CNV data in later chapter. In \cref{chapter:multiodality}, we extended the single modality based predictive models to multimodality-based cancer typing method by employing a multimodal convolutional autoencoder classifier, but with a focus of breast cancer. 

\hspace*{5mm} Inspired by the results in \cref{chapter:multiodality}, we employed two different approaches to open the `black-box' unimodal and multimodal models in \cref{chapter:xai} towards making them predictions of cancer types explainable. We identified significant biomarkers and computed their feature importance and exposed top-k gene biomarkers. As we managed to open several `black box' models and made them `white-box' non-trivially, we could already deploy them for initial clinical diagnosis. However, applying different types of adversarial attacks on the models and assessing their robustness was crucial, which we did in In \cref{chapter:robustness}. We performed different types of attacks, including image content moderation, numeric data moderation, and out-of-distribution, followed by assessing the robustness of each model against these scenario. Up to now, these chapters have provided the basic foundation for this this dissertation. 
\begin{table}[h!]
    \caption{Research questions and how did we answer them in this thesis}
    \label{table:rq_answers}
    \centering
    \vspace{-2mm}
    \scriptsize{
    \begin{tabular}{|p{4.8cm}|p{10cm}|}
         \hline
         \multicolumn{2}{|c|}{Research questions answered in this thesis} \\
         \hline
         \textbf{Research question} & \textbf{Approach} \\
         \hline
         \begin{flushleft} \textbf{RQ1}: How to use multimodal data to provide a clinical diagnostic decision by accurately predicting an outcome? \end{flushleft}& Since multiple factors are involved~(e.g., estrogen receptor, progesterone receptor, and human epidermal growth factor receptor statuses for breast cancer diagnosis), multimodal feature representation is learned by convolutional autoencoder architectures from gene expression, miRNA expression, CNVs, and and clinical outcome to provide more accurate clinical diagnostic decision. Nevertheless, a neural ensemble method by combining several deep architectures can be more effective than structures solely based on a single model, the final trained model generalize well by reducing variance and bias.\\ 
         \hline
         \begin{flushleft}\textbf{RQ2}: How to identify relevant variables or factors~(i.e., features, biomarkers for cancer genomics, for example) that contributed most for a certain clinical diagnostic decision?\end{flushleft} & To explain why did a model behave in a certain way, we trained several robust neural network architectures in which different interpretable feature attribution methods~(e.g. SHAP) and explainable logic~(e.g., Grad-CAM, Grad-CAM++, attention mechanism, and LRP) are embedded. The former can identify most significant biomarkers giving the top-10 genes for each cancer type and common genes across types. The latter can provide class-specific explanations by highlighting class-discriminating images region, where a cancer sample is embedded into 2D gray-scale image. Since, lot's of gene biomarkers are not significant making them weak features. The convolutional autoencoder-based representation learning can extract most significant features for the classification.\\ 
         \hline
         \begin{flushleft}\textbf{RQ3}: How to provide human-understandable interpretations of the predictions using decision rules?\end{flushleft} & Interpretable rules can be deduced by combining the reasoning and the prediction from the model. We have seen that explaining the predictions with plots and charts are good for exploration and discovery but interpreting them for the first time may be difficult. Where a decision rule of the prediction can help explain innumerable factors to determine the likelihood of a patient needing certain treatment. \\ 
         \hline
         \begin{flushleft}\textbf{RQ4}: How to disseminate and validate embedded domain knowledge~(e.g., molecular mechanisms of carcinogenesis)? \end{flushleft}& We had a semantic layer consist of a instance classifier and an ontology reasoner. To ensure that the semantic layer communicates with the lower layer, we developed a domain-specific cancer ontology by combining metadata and other external sources. Where based on the biomarkers and their attributes, the reasoner decides whether a biological entity is of correct types based on the ontological reasoning. The reasoner also help validate the findings and decision rules in order to deduce human-understandable decision rules.\\ 
         \hline
         \begin{flushleft}\textbf{RQ5}: How to score a black-box model on fairness and transparency?\end{flushleft} & We assessed our CDSS both from statistical and philosophical perspective. We tried to mitigate different types of bias such as sampling, reporting, and model selection bias. Besides, we employed SHAP and random forest classifier on individual modality and try to understand clinical feature contributions to support the statistical feature importance. Nevertheless, we combined the predictions with the reasoning towards generating human-interpretable decision rules.\\ 
         \hline
    \end{tabular}}
\end{table}
    
\hspace*{5mm} Now on top of the foundation, in \cref{chapter:xai_rules}, we generated decision rules by combining model predictions and interpretations based on RuleMatrix library. We then explain those decision rules to end users via an interactive visual interface. In \cref{chapter:nsr}, we developed a domain-specific cancer ontology by combining metadata and other external sources. Where based on the biomarkers and their attributes, the reasoner decides whether a biological entity is of correct types based on the ontological reasoning. Where the semantic layer communicates with the lower layer, i.e., ontology. The reasoner also help validate the findings and decision rules in order to deduce human-understandable decision rules in \cref{chapter:xai_rules}. In \cref{chapter:fairness}, we assess both explainability and fairness of our approach from statistical and philosophical perspective.  While developing our explainable clinical decision support system by improving the fairness and transparency, this thesis attempted to solve several research questions as outlined in \cref{table:rq_answers}.

\section{Potential Limitations}
One question to be answered after reading this dissertation is whether our approach is enough explainable and fair, as the title might suggest. The answer to this question is `partially yes' as outlined in different chapters. Further, we outlined potential limitations of our studies:  

\begin{itemize}[noitemsep]
    \item First of all, in some chapters utilization of full data in multimodal network setting was not feasible, at least due to study focus. For example, we generated the decision rules from individual single modality and explained them to end users. This is not desirable. 
    \item Secondly, one major limitation of the presented approach is that the method has not been validated with a real expert in specific do-mains~(e.g., health care). We expect to specialize the proposed method to meet the needs of specific domain problems~(e.g., cancer diagnosis,or credit approvals) based on future collaborations with domain experts. 
    \item Secondly, this dissertation lacks of systematically study the advantages and disadvantages of different knowledge representations~(e.g.,decision trees and rule sets) when considering human understand ability. In other words, would people feel more comfortable with hierarchical representations~(e.g., decision trees) or flat representations~(e.g., plots or lists) under different scenarios~(e.g., verifying a prediction or understanding a complete model)?
    \item Thirdly, as we argued that explaining predictions with plots and charts are useful for exploration and discovery\cite{KarimIEEEAccess2019}. However, explaining them to patients may still be tedious if the rules goes very long, which may require in more human-interpretable natural language. 
    %\item Fourthly, 
\end{itemize}

%We regard this work as a preliminary and exploratory step towards explainable machine learning and plan to further extend and validate the idea of interpretability via inductive rules. 

\section{On Automatic Diagnostic Decision}
AI-based systems have been utilized in numerous scenarios emphasizing biomedical imaging, including automated diagnosis and treatment in clinical settings. However, if we cannot see how a clinical decision was made, we cannot know what impact it will create on a patient because the day when such AI-guided systems will make life decisions for humans is not very far ahead. As outlined, current approaches not only suffer from opaqueness but also merely consider domain knowledge~(apart from expert radiologists and oncologists), e.g., scientific literature and knowledge bases. 

\hspace*{5mm} Nevertheless, a fully automated method without the possibility for human verification would be unconscionable and potentially dangerous in the current clinical setting. Further, as Curtis Langlotz\footnote{\url{https://www.nature.com/articles/d41586-019-03847-z}} stated ``AI won't replace radiologists, but radiologists who use AI will replace radiologists who don't''. In the same line, we would argue that our approach is not to replace a doctor but to be evaluated in a clinical setting and is by no means a suitable replacement for a human oncologist. We would even argue that human judgement is indispensable when the life of patients is at stake. However, still we hope our findings will be a useful contribution science as well as towards an increasing acceptance and adoption of AI-assisted applications in the clinical practice. 

\section{Future Directions}
We want to outline potential areas of enhancement. Firstly, finding additional insights about the disease by predicting the risk of certain disease and finding links between the genetic and biological properties of conditions and the composition action of drugs accurately with transparency and fairness is an utmost necessity, so that better treatment or prognosis can be provided. Secondly, providing pre- and post-diagnosis explanations in which human-interpretable explanations about the outcomes and making the diagnosis transparent and fair by minimizing the prediction biases. Thirdly, a lot of what we know about drugs, genes, protein, viruses, and their mechanism is spread across a huge number of scientific articles. Research initiatives also now gradually adopting semantic web technologies such as knowledge graphs~(KG) and domain-specific ontologies as a means of building structured networks of interconnected knowledge about drugs, viruses, bacteria, genes, and proteins. Those articles could be used as a large-scale knowledge source and be be of huge impactful in disseminating knowledge towards. In particular, we can apply NLP and ML techniques to scientific literature to construct a biomedical KG. The resultant KG will provide a comprehensive view of mechanism of carcinogenics, showing how the different entities~(e.g., drugs, viruses, bacteria, genes, and proteins) are connected exposing their biological pathways. 

\hspace*{5mm} Subsequently, we can model the KG as the resource description framework (RDF), followed by embeddings the nodes and their relations in a lower-dimensional vector space. Then the embeddings can be used in different link prediction tasks such as predicting drug-target interactions (e.g., finding the connection between the genes and the potential drug candidates). Finally, using the integrated KG for both link prediction and ontological reasoning could be an option. Such that the symbolic reasoning based on the KG will be used to provide interactive explanations against human query and reasoning mechanisms based on a knowledge matching process, such that the reasoner can characterize errors w.r.t hierarchical relations from the KG to give reasons and explanations about a specific prediction by minimizing the prediction biases. In future, we intend to overcome above-mentioned limitations for cancer genomics as well as for other domain by alleviating more data and incorporating domain knowledge with neuro-symbolic reasoning~(where the model will be directly guided or at least manipulated) to generate decision rules to make the diagnosis fairer. 