\chapter{Foundations}
\label{chapter:preli}
\textit{``The beginning of knowledge is the discovery of something we do not understand''}- Frank Herbert 

\section{Chapter Overview}
In this chapter, we cover the foundations and concepts that will be found in all the subsequent chapters. We start with a basic of decision support systems~(DSS) and move towards an intelligent DSS. Subsequently, we cover the necessary foundations for building an explainable DSS for to provide accurate diagnoses decisions about different cancer types. In particular, we cover in detail the concepts of machine learning~(ML) and deep learning~(DL), artificial intelligence~(AI), decision rules, neuro-symbolic reasoning, and representation learning~(RL), with a focus on explainability and algorithmic transparency. We also discuss cancer types, subtypes, and growth and metastasis of cancer w.r.t biological interpretations of genomics data. 

\section{Decision Support Systems}\label{sec:DSS}
Decision support systems~(DSS) are information system that support business or organizational decision-making activities~\cite{hackathorn1981organizational}. A typical DSS takes a set of input and analyzes them to generate actionable insights and recommendation based on which `decisions' are generated. 
In other words, DSSs are often computerized program that analyse massive amounts of data to assists in decision-making. 
However, a real-world DSS may require additional consideration and analysis of multiple criteria features which, in turn, affect the final decisions.  Criteria are often conflicting in nature. For example, in the case of investing in share market, factors such as companies profile and current trend, price, market volatility, and socio-economic or political situation all come into consideration. 

\hspace*{3.5mm} Therefore, to improve the quality and accuracy of the intended decisions, scientific approaches often necessary to perform complex evaluation and validation in the critical decision-making process. Nevertheless, a decision may need be decision can be further validated with user knowledge and expertise. Knowledge base~(KB), predictive models~(decision context and user criteria), and the user interface are three fundamental components of a DSS architecture~\cite{hackathorn1981organizational}. Besides, users themselves play important role in the overall decision-making process, particularly by providing input and out specification and domain-knowledge in the validation stage. DSS components can be broadly classified as follows~\cite{hackathorn1981organizational}: 

\iffalse 
\vspace{-2mm}
\begin{enumerate}[noitemsep]
    \item  \textbf{Inputs}: Factors, numbers, and characteristics to analyze.
    \item \textbf{Knowledge base}: Inputs requiring manual analysis by the user.
    \item \textbf{Outputs}: Transformed data from which DSS "decisions" are generated.
    \item \textbf{Decisions}: Results generated by the DSS based on user criteria.
\end{enumerate}
\vspace{-2mm}
\fi 

Haettenschwiler et al.~\cite{haettenschwiler1999neues} differentiates three types of DSS called passive, active, and cooperative DSS:  

\vspace{-2mm}
\begin{itemize}[noitemsep]
    \item \textbf{Passive DSS} - help in the decision-making process, but cannot provide explicit decision suggestions.
    \item \textbf{Active DSS} - provides decision suggestions or solutions. 
    \item \textbf{Cooperative DSS} - allows iterative process between human and the DSS system itself towards achieving a consolidated solution.
\end{itemize}
\vspace{-2mm}

\hspace*{3.5mm} A cooperative system, thus, has several advantages over active and passive DSS. For example, using a cooperative DSS, decision makers can modify or refine the decision suggestions provided by the system, before sending them back to the system for validation~\cite{hackathorn1981organizational}. Subsequently, the system again improves, completes, and refines the suggestions of the decision maker and sends them back to them for validation. Secondly, the presence of human-in-the-loop, help improve human-AI interaction towards achiving an intelligent DSS. In terms of mode of assistance, communication-driven, data-driven, document-driven, knowledge-driven, and model-driven DSSs can be developed~\cite{power2002decision}:

\vspace{-2mm}
\begin{itemize}[noitemsep]
    \item \textbf{Communication-driven DSS} - used to leverage supports for multiple users working on a shared task collaboratively, particularly targeting at internal teams and their partners. Web or client server via chats and instant messaging software are most common technology used to deploy to leverage online collaboration and net-meeting systems like Google Docs. 
    \item \textbf{Data-driven DSS} - used to provide access to and manipulation of internal or external company data, such that organizational staff can query a database or data warehouse to seek specific answers to specific questions. This types of DSSS were conventionally deployed via a mainframe computer, client-server system, or on the web. 
    \item \textbf{Document-driven DSS} - used to search, manage, retrieve, and manipulate unstructured information via web pages~(i.e., often client-server architecture) that are stored in a variety of formats. 
    \item \textbf{Knowledge-driven DSS} - used to provide specialized problem-solving expertise based on domain-knowledge. In knowledge-driven DSSS, facts, rules, procedures or in similar structures like interactive decision trees and flowcharts, often form the foundation of the KB. 
    \item \textbf{Model-driven DSS} - used to leverage access to and manipulation of a statistical, financial, or simulation model. Model-driven DSS use data and parameters provided by users to assist decision makers in analyzing a situation. However, they are not necessarily data-intensive. 
\end{itemize}
\vspace{-2mm}
%However, DNN models are perceived mostly as `black box' methods because of their not well-understood internal functioning. Besides, they cannot reason their underlying decisions, leaving them incapable of aiding transparent and trustworthy decisions. Consequently, we call such DSS a `black box' model. 

%This chapter provides a general introduction to this dissertation, provides the motivation for this research, list down the hypotheses and research questions, and outlines the structure of this thesis. 

\hspace*{3.5mm} The expert systems based on collections of `if-then' rules were the dominant technology for AI in the 1980s~\cite{davenport2019potential}. They were widely employed for `clinical decision support system~(CDSS) purposes over the last couple of decades. Such a DSS require human experts and domain knowledge to generate a set of rules used to provide decision in numerous tasks~\cite{davenport2019potential}. 
As long as there are a few rules in the rule set, they work well and easy to understand. However, when the number of rules is large, including conflicting rules, they tend to break down. Moreover, if the underlying domain knowledge changes, updating the rules can be difficult and time consuming~\cite{davenport2019potential}. Moreover, rule-based clinical DSS are difficult to maintain as medical knowledge changes and are often not able to handle the explosion of data and knowledge based on genomic, proteomic, metabolic and other ‘omic-based’ approaches to care~\cite{das2020opportunities}.
Subsequently, they gradually being replaced with ML-based systems with AI capabilities. 

\hspace*{3.5mm} DSSs that can perform selected cognitive decision-making functions based on artificial intelligence~(AI) or intelligent agents technologies called intelligent DSS. Due to recent advancement and performance across domains like computer vision, natural language processing, multimedia analytics, and business analytics, an AI-guided DSS could eventually be applied to various automated decision-making processes~\cite{davenport2019potential}.  In an intelligent DSS, a set of learning algorithms are embedded to perform cognitive decision-making in which ML often act as one of the most common forms of AI~\cite{das2020opportunities}. An intelligent DSS is often ML or DL-based DSS, where a ML or DL algorithm is employed to improve the learning outcome so that the decision-making process becomes automatic by reducing the level of human interaction as much as possible~\cite{davenport2019potential}. 

\hspace*{3.5mm} This thesis aims at developing an DSS by leveraging both data-, knowledge-, and model-driven characteristic, such that an ML model trained on the data forms the foundation of the DSS. Then the decision making process and the domain-knowledge will be used to aid the users to validate the outcome. Since the `black-box' nature and opaqueness in predictive model raises numerous legal, ethical, and practical concerns, we aim to improve the transparency and explainability of the DSS. In the subsequent sections, fundamental concepts of ML, multimodal learning, and interpretability are discussed along with different types of data that required. % to develop the DSS such as  required. 

\section{Machine Learning}
ML is using a set of statistical and mathematical algorithms to solve tasks such as concept learning, predictive modeling, clustering, and mining useful patterns. Learners or algorithm are a program used to learn a ML model from data. An ML model is a learned program that maps inputs to predictions. Tom Mitchell~\cite{mitchell1997machine} defined machine learning as \textit{``a computer program is said to learn from experience $E$ with respect to some class of tasks $T$ and performance measure $P$, if its performance at tasks in $T$, as measured by $P$, improves with experience $E$"}. This definition can be interpreted as follows: based on a set of observation or data~(i.e., experience $E$), a learning algorithm can be used to solve a problem~(i.e., task $T$) iteratively until the performance~(i.e., $P$) is not satisfactory. 

\hspace*{3.5mm} Based on this interpretation, a machine can learn from data/observations and can be improved with experience to predict an outcome more accurately. When it comes to data, what we mean is a set of features/inputs and targets from which the machine learns. Subsequently, a learned program called a ML model can map inputs to predictions. The resultant is weights for a linear model or for a neural network~(we'll shortly discuss about). Training a ML model is an optimization problem, where the objective is finding a minimizer of a convex function $f$ and a weight vector $w$ on $d$ data points. Subsequently, we can formulate the learning task as $\min _{w \in \mathbb{R}^{d}} f(w)$, where the objective function is as follows~\cite{karim2018scala}:

\vspace{-4mm}
\begin{align}
    f(w):=\lambda R(w)+\frac{1}{n} \sum_{i=1}^{n} L\left(w ; x_{i}, y_{i}\right)
\end{align}

\hspace*{3.5mm} We call the method linear if $L(w;x,y)$ can be expressed as a function of $w^Tx$ and $y$. The objective function $f$ has two components, i) a regularizer that controls the complexity of the model, ii) the loss that measures the error of the model on the training data. The loss function $L(w;)$ is typically a convex function in $w$. The fixed regularization parameter $\lambda \geq 0$ defines the trade-off between minimizing the loss on the training error and minimizing model complexity to avoid overfitting~\cite{karim2018scala}. If both of components are convex, their sum is also convex, else non-convex~\cite{zaccone2018deep}. Therefore, using a convex optimization technique, we can minimize the function until it converges towards the minimum error.  %This can be a set of weights for a linear model or for a neural network. Other names for the rather unspecific word "model" are "predictor" or - depending on the task - "classifier" or "regression model". he trained ML model is called $\hat{f}$ or $\hat{f}(x)$

\begin{figure}[h]
	\centering
	\includegraphics[scale=0.6]{images/B08452_01_02.png}
	\caption{Lifecycle of a typical  pipeline involve in an ML system~\cite{karimScalaML2019}} 
	\label{fig:ml_pipeline}
\end{figure}

\hspace*{3.5mm} The regularization parameter defines the trade-off between minimizing the training error and the model's complexity in an effort to avoid overfitting problems. More elaborately, when using an ML algorithm, the goal is to obtain the best hyperparameters of a function that return the minimum error when making predictions. 
%Given that a problem is convex, it is usually easier to analyze the asymptotic behavior of the algorithm, which shows how fast it converges as the model observes more and more training data. 
The challenge of ML is to allow training a model so that it can recognize complex patterns and make decisions not only in an automated way but also as intelligently as possible. 
However, if the performance is not satisfactory, additional tuning or retraining would be necessary to get the best model. %, before % based on hyperparameter optimization.
%deploying the best model. % in a production-ready environment. 
\Cref{fig:ml_pipeline} summarizes these steps, where an instance and prediction can be described as: 

\begin{itemize}[noitemsep]
    \item \textbf{An instance} is a row in the dataset~(also called data point, example, or observation). An instance consists of the features $x^{(i)}$ and~(if known) the target outcome ${y}$, where target is the information that the model learned to predict.

    \item \textbf{Prediction} is what a ML model ``guesses" what the target value should be based on the given feature, i.e., $\hat{y}$ = $\hat{f}\left(x^{(i)}\right)$. If $\hat{y}$ is equal to~(or close) ${y}$, we say it's an accurate prediction.   
\end{itemize}

%\subsection{Machine learning tasks}
\hspace*{3.5mm} Although every ML problem is an optimization problem, the way they are solved can vary. Depending on the data type~(often combination of features and targets), learning tasks could be of different three types such as supervised, unsupervised, and reinforcement learning. Supervised learning is the simplest and most well-known automatic learning task. It is based on a number of predefined examples, in which the category to which each of the inputs should belong is already known, as shown in \cref{fig:ml_pipeline_sup}:

\begin{figure}[h]
	\centering
	\includegraphics[scale=0.7]{images/sup.png}
	\caption{Workflow of supervised learning technique~\cite{karimScalaML2019}} 
	\label{fig:ml_pipeline_sup}
\end{figure}

\hspace*{3.5mm} In the preceding diagram, an actor~(e.g., data scientist) performs extraction, transformation, and load~(ETL) and necessary feature engineering~(such as feature extraction, selection, etc.) to get the appropriate data consisting of features and labels, to fed into the model. The training set is used to train a ML model and the validation set is used to validate the training against the overfitting problem and regularization. Then the actor can evaluate model's performance on the test set~(i.e., unseen data). The supervised learning context includes classification and regression. While classification is used to predict which class a data point is a part of discrete value or the label of the class attribute, whereas regression is used for predicting continuous values and making a numeric prediction of the class attribute.

\hspace*{3.5mm} How would you summarize and group a dataset if the labels were not given? Probably, you'll try to answer this question by finding the underlying structure of a dataset and measuring the statistical properties such as frequency distribution, mean, standard deviation, and so on. If the question is how would you effectively represent data in a compressed format? You'll probably reply saying that you'll use some software for doing the compression, although you might have no idea how that software would do it. \Cref{fig:ml_pipeline_unsup} shows the typical workflow of an unsupervised learning task. 

\begin{figure}[h]
	\centering
	\includegraphics[scale=0.9]{images/unsup.png}
	\caption{Workflow of unsupervised learning technique~\cite{karimScalaML2019}} 
	\label{fig:ml_pipeline_unsup}
\end{figure}

\hspace*{3.5mm} On the other hand, reinforcement learning focuses on the learning of the system through its interactions with the environment, where system's parameters are adapted based on the feedback obtained from the environment, which in turn provides feedback on the decisions made by the system. On the other hand, another most complex forms of ML involve deep learning~(DL), or deep neural network~(DNN) models with many levels of features or variables that predict outcomes that help take meaningful decision. We will discuss about DL and DNN in subsequent sections. 

%The following diagram shows a person making decisions in order to arrive at their destination. 

%After necessary preprocessing and feature engineering, a common practice is splitting the input data into 60\% for the training, 10\% for the validation, and 20\% for testing, but it depends on use cases. \hspace*{3.5mm} Besides, up-sampling or down-sampling may require in case of class imbalanced scenario. In practice and from the application point of view, a typical ML application involves several processing steps, from the input to the output, forming a scientific workflow as shown \cref{fig:ml_pipeline}. 

\iffalse
\subsection{Role of data in machine learning}
Data in any ML pipeline is a first-class citizen and play key role. The performance of any learning depends on quality features. Depending upon the learning tasks, often we may have to split the data, e.g., in supervised learning tasks, the entire learning process requires input datasets to split into different sets~\cite{karim2018java}:

\vspace{-1mm}
\begin{itemize}[noitemsep]
    \item \textbf{Training set:} is the knowledge base coming from historical or live data used to fit the parameters of the ML algorithm. During the training, the model utilizes the training set to find optimal weights of the network and reach the objective function by minimizing the training error~\cite{karim2018java}. 
    \item \textbf{Validation set:} is a set of examples used to tune the parameters of an ML model to ensure that the model generalizes towards avoiding overfitting~\cite{karim2018java}. 
    \item \textbf{Test set:} is used for evaluating the performance of the trained model on unseen data. This step is also referred to as model inferencing. After assessing the final model on the test set, no need to tune the model before deploying in a production-ready environment~\cite{karim2018java}.
\end{itemize}
\vspace{-1mm}

\hspace*{3.5mm} In the context of supervised learning, the learning process required for the input dataset is split randomly into three sets, for example, $60\%$ for the training set, $10\%$ for the validation set, and the remaining $30\%$ for the testing set.
\fi 

\section{Deep Learning and Neural Networks}
Deep neural networks~(DNNs) form the core of deep learning~(DL) by providing a set of algorithms to model complex and high-level abstractions in data and can better exploit large-scale datasets to build complex models, and have been widely used in computer vision, speech recognition, NLP, social network, machine translation, bioinformatics, etc. Layers in a neural network is composed of a set of artificial neurons called perceptron~\cite{yuan2019adversarial}. A perceptron maps a set of inputs to outputs with a special function called activation functions. Multiple such layers are stacked to create such a neural network. 

\hspace*{3.5mm} Till date, numerous neural network architectures have been proposed and are in use. We can categorize DL architectures into four groups: i) feed-forward neural networks~(FFNNs), ii) Convolutional neural networks~(CNNs), iii) Recurrent neural networks~(RNNs), iv) Autoencoders~(AEs), and v) Emergent architectures. However, FFNNs, CNNs, AEs, and RNNs have many improved variants. Although most of the variants are proposed or developed for solving domain-specific research problems, the basic working principles still follow the original  architectures. In this section, we provide preliminaries and theoretical foundations of these architectures that are used throughout in the subsequent chapters.  

\subsection{Feed-forward neural networks}
Feed-forward neural networks~(FFNNs) are neural networks that have a complex and deeper architecture with a large number of neurons in each layer, and many connections between them. Although DNN refers to a very deep network, for simplicity, we consider MLP, stacked autoencoder~(SAE), and deep belief networks~(DBNs)~\cite{Hinton:2009} as DNN architectures. These architectures mostly work as an FFNN, meaning information propagates from input to output layers. Multiple perceptrons are stacked together as MLPs, where layers are connected as a directed graph. Fundamentally, an MLP is one of the most simple FFNNs since it has three layers as shown in \cref{fig:mlp_1}: an input layer, a hidden layer, and an output layer, where the signal propagates one way, from the input layer to the output layer as shown in \cref{fig:mlp_1}.

\hspace*{3.5mm} However, it is not possible to determine a priori, with adequate precision, the required number of hidden layers, or even the number of neurons that must be contained inside each hidden layer to compute a non-linear function. There is no straightforward answer to this, but we can try increasing the number of neurons in a hidden layer until the FFNN starts overfitting. We will discuss this later on. Despite some rules of thumb, setting the number of hidden layers relies on experience and on some heuristics to determine the structure of the network. If a low number of hidden layers, or neurons, constitute the neural network architecture, then the network is not able to approximate with adequate precision the unknown function, for example. This could be because it is too complex, or because the backpropagation algorithm falls within a local minimum. If the network is composed of a high number of hidden layers, then we have an overfitting problem. One solution to this problem is regularization through dropout. Therefore, a complex network can consist of many neurons, hidden layers, and connections, but in general, an ANN with two or more hidden layers is called a DNN. From the implementation perspective, a DNN can be constructed by stacking multiple ANNs together~\cite{karimDLTF2018}.

\begin{figure}[h]
    \centering
    \includegraphics[scale=0.7]{images/ffnn_1.png}
    \caption[Architecture of multilayer perceptron~(MLP)]{The architecture of a MLP with an input layer, 2 hidden layers, and an output layer}
    \label{fig:mlp_1}
\end{figure}

\hspace*{3.5mm} RBMs are the basic building blocks for DBNs. Unlike MLP, which is an FFNN that's trained in a supervised way, DBNs are trained in two phases: unsupervised pretraining and supervised fine-tuning. In unsupervised pretraining, layers are stacked in order and trained in a layer-wise manner with used unlabeled data. In supervised fine-tuning, an output classifier layer is stacked and the complete neural network is optimized by retraining with labeled data. One problem with MLP is that it often overfits the data, so it doesn't generalize well. To overcome this issue, DBN was proposed, which uses a greedy, layer-by-layer, pretraining algorithm and composed of a visible layer and multiple hidden unit layers. However, despite numerous successes, DBNs have been replaced with AEs. 

\subsection{Autoencoders}
\label{preli:AEs}
A regular AE consists of multi-layer dense networks called encoder and decoder, which is architecturally an MLP~\cite{karimDLTF2018}. AE also acts data compression technique where the compression and decompression functions are data-specific, lossy, and learned automatically from samples rather than human-crafted manual features~\cite{karimDLTF2018}. The encoder learns the representation of input $X$ in a compressed format in which the data is mapped and transformed into an embedding $Z$.  
%The decoder module of the AE network maps $Z$ to the reconstruction $X^{\prime}$~\cite{karimACCESS2019}, i.e., by approximating a function, $h_{W, b}(x) \approx x$, the decoder module to the identity function in order to output $\chi^{\wedge}$, which is similar original input x. 
Then the decoder tries to reconstruct $X$ from $Z$ by reducing the reconstruction loss between $X$ and its corresponding reconstruction $X^{\prime} \in \mathbb{R}^{D}$ such that useful information is not lost in the encoding phase~\cite{KarimIEEEAccess2019}.

\vspace{-2mm}
\begin{itemize}[noitemsep]
    \item \textbf{Encoder}: encodes the input into a latent-space representation using function $h = f(X)$.
    \item \textbf{Decoder}: reconstructs the input from the latent space representation using function $\tilde{x} = g(h)$. 
\end{itemize}
\vspace{-2mm}

\begin{figure}[h]
    \centering
    \includegraphics[scale=0.6]{images/ae.png}
    \caption{An unsupervised autoencoder as a network for latent feature learning~\cite{karimDLTF2018}}
    \label{fig:ae_theory1}
    \vspace{-2mm}
\end{figure}

Usually, reconstruction loss is the distance measure~($d_{AE}$) between input $x_i$ and network's output $f(x_i)$: 

\vspace{-2mm}
\begin{equation}
    L_{AE}=\text{$d_{AE}$}(x_i, f(x_i) = \sum_{i} ||x_{i}-f(x_i)||^{2}.
    \label{eq:Loss1}
\end{equation}

\hspace*{3.5mm} The identity function seems a particularly trivial function to be trying to learn, but by placing constraints on the network, such as by limiting the number of hidden units, we can discover interesting features of the data~\cite{karimDLTF2018}. Although used for learning representations from numeric data, AE is mostly not suitable for 2D/3D finite and discrete signals or digital images~\cite{min2018survey}, primarily because of their weak RL capability. In \cref{sec:rep_learn}, we discuss different variants of autoencoders for the RL. %, which we will use in subsequent chapters. 

\subsection{Convolutional neural networks}
DNNs have no prior knowledge of how the pixels are organized because they do not know that nearby pixels are close. CNNs embed this prior knowledge using lower layers by using feature maps in small areas of the image, while the higher layers combine lower-level features into larger features. This setting works well with most of the natural images, giving CNN a decisive head start over DNNs~\cite{karimIoT2019}. CNNs have achieved much and have been widely adopted in computer vision, e.g., image recognition. The connection schemes in a CNN are significantly different compared to an MLP or DBN. A few of the convolutional~(conv) layers are connected in a cascade style. 

\begin{figure}[h]
    \centering
    \includegraphics[scale=0.6]{images/cnn.png}
    \caption{A schematic architecture of a CNN used for facial recognition~\cite{karim2017predictive,zaccone2018deep}}
    \label{fig:cnn_theory1}
\end{figure}

\hspace*{3.5mm} The output from each convolutional layer is a set of objects, called feature maps generated by a single kernel filter. Then, the feature maps can be used to define a new input to the next layer. Each neuron in a CNN network produces an output followed by an activation threshold, which is proportional to the input and not bound. Each layer is backed up by an ReLU layer, a pooling layer, additional convolutional layers (+ReLU), and another pooling layer, which is followed by a fully connected layer~(FCL) and a softmax layer. The preceding diagram~(\cref{fig:cnn_theory1}) is a schematic architecture of a CNN used for facial recognition, which takes facial images as input and predicts emotions such as anger, disgust, fear, happy, and sad~\cite{karimDLTF2018}.  
%A neuron can be active (or firing) if its output value is close to 1, or inactive if its output value is close to 0. However, for simplicity, we assume that the neurons are inactive most of the time. This argument is true as long as we are talking about the sigmoid activation function. However, if you are using the tanh function as an activation function, then a neuron is inactive when it outputs values close to -1.

\subsection{Recurrent neural networks}
Traditional neural networks instead ignore past events, as it is not possible for a neural network to use past scenes to classify current ones. In contrast to conventional neural networks, RNNs are networks with a loop that allows the information to be persistent in a neural network. The backpropagation time rolls out the RNN, creating a very deep feed-forward neural network. The impossibility of getting a long-term context from the RNN is due precisely to this phenomenon: if the gradient vanishes or explodes within a few layers, the network will not be able to learn high temporal distance relationships between the data. 

\begin{figure}[h]
    \centering
    \includegraphics[scale=0.3]{images/B09698_06_4.png}
    \caption{In RNN all the weights in all the layers have to be learned with time~\cite{karimDLTF2018}}
    \label{fig:rnn_theory1}
    \vspace{-2mm}
\end{figure}

\hspace*{3.5mm} The preceding diagram~(\cref{fig:rnn_theory1}) is a schematic architecture of a RNN, where all the weights in all the layers have to be learned with time~\cite{karimDLTF2018}. However, computed and back-propagated gradient tends to decrease~(or increase) at each instant of time and then, after a certain number of instants of time, the cost function tends to converge to zero~(or explode to infinity). However, improved RNN variants such as LSTM and GRU can combat the vanishing gradients and offers excellent results and performance. LSTM-based networks are ideal for the prediction and classification of temporal sequences. As the name signifies, that short-term patterns are not forgotten in the long term. 

\subsection{Emergent architectures}
Many other emergent DL architectures have been suggested, such as Redidual networks~(ResNets), Deep SpatioTemporal Neural Networks~(DST-NNs), Multi-Dimensional Recurrent Neural Networks~(MD-RNNs), and densely-connected neural networks~(DenseNet). Other neural networks such as CapsNets - an improved version of a CNN, RNN for image recognition, and Generative Adversarial Networks~(GANs) for simple image generation. Apart from these, factorization machines for penalization and DL are also used. 

\iffalse
\subsubsection{Residual neural networks}
Since there are sometimes millions and millions of hyperparameters and other practical aspects, it's really difficult to train deeper neural networks. To overcome this limitation, a residual learning framework is proposed~\cite{zagoruyko2016wide} to ease the training of networks that are substantially deeper than those used previously. They also explicitly reformulated the layers as learning residual functions with reference to the layer inputs, instead of learning non-referenced functions. This way, these residual networks are easier to optimize and can gain accuracy from considerably increased depth. The downside is that building a network by simply stacking residual blocks inevitably limits the optimization ability. %To overcome this limitation, Ke Zhang et al. also proposed using a multilevel residual network.

\subsubsection{Generative adversarial networks}
GANs architectures consist of two networks pitted against each other introduced by Ian Goodfellow et al.~\cite{GAN}. In GANs, two main components are the generator and discriminator. In a GAN architecture, a generator and a discriminator are pitted against each other—hence the name - adversarial~\cite{GAN}:

\vspace{-2mm}
\begin{itemize}[noitemsep]
    \item \textbf{Generator}: the generator tries to generate data samples out of a specific probability distribution and is very similar to the actual object.
    \item \textbf{Discriminator}: the discriminator will judge whether its input is coming from the original training set or from the generator part. 
\end{itemize}

\vspace{-2mm}
\begin{figure}[h]
    \centering
    \vspace{-3mm}
    \includegraphics[width=0.75\textwidth,height=50mm]{images/gan.png}
    \caption{Schematic diagram of a simple GAN~\cite{karimDLTF2018}}
    \label{fig:gan}
    \vspace{-2mm}
\end{figure}

\hspace*{3.5mm} \Cref{fig:gan} shows a schematic diagram of a simple GAN. Many DL practitioners think that GANs were one of the most important advancements because GANs can be used to mimic any distribution of data, and, based on the data distribution, they can be taught to create robot artist/super-resolution images, text to image synthesis, speech. Facebook's AI research director, Yann LeCun, thinks GANs are the most interesting idea in the last 10 years of ML\footnote{See: Generative Adversarial Networks: What Are They and Why We Should Be Afraid, by Thomas Klimek, 2018}.

\subsubsection{Capsule networks}
In CNNs, each layer understands an image at a much more granular level through a slow receptive field or max pooling operations. If the images have rotation, tilt, or very different shapes or orientation, CNNs fail to extract such spatial information and show very poor performance at image processing tasks. Even the pooling operations in CNNs cannot be much help against such positional invariance. This issue in CNNs has led CapsNet~\cite{CapsNet}, where a capsule is a group of neurons whose activity vector represents the instantiating parameters of a specific type of entity, such as an object or an object part~\cite{CapsNet}.

\begin{figure}[h]
    \centering
    \includegraphics[width=0.9\textwidth,height=40mm]{images/capsnet.png}
    \caption{Schematic diagram of a simple three-layer CapsNet~\cite{karimDLTF2018}}
    \label{fig:capsnet}
\end{figure}

\hspace*{3.5mm} Unlike a regular DNN, where we keep on adding layers, in CapsNet, the idea is to add more layers inside a single layer, making the CapsNet a nested set of neural layers~\cite{CapsNet}. In CapsNet, the vector inputs and outputs of a capsule are computed using the routing algorithm used in physics, which iteratively transfers information and processes the self-consistent field~(SCF) procedure. \Cref{fig:capsnet} shows a schematic diagram of a simple three-layer CapsNet. The length of the activity vector of each capsule in the DigiCaps layer indicates the presence of an instance of each class, which is used to calculate the loss. 
\fi 

\section{Representation Learning}
\label{sec:rep_learn} 
In recent years, DL has been successfully applied to multimodal learning problems, with the aim of learning useful joint representations from different types of data called modality. Modality refers to the way in which we experience a real-life events that includes multiple such modalities~\cite{mmsurvey}. For example, in time series domain available modalities includes video, audio, and sensor signals.
Second example would be visual question answering system or an e-commerce site, where both images and text need to be combined. For the former, it is imperative to consider the temporal structure of individual modalities during the fusion process. For the latter, both images and text descriptions are provided for the customers. 

\hspace*{3.5mm}Representing raw data in a format that a ML model can work with has always been a big challenge in order to develop DSS~\cite{mmsurvey}. Images, audio samples, individual words, or a sentence features are often represented in vector or tensor formats, which is called representation learning~(RL). In many cases, using a single unimodal representation learning is sufficient for each modality in the creation of multimodal space. However, multimodal representations are distributed vectors that map multiple modes of information to a single mathematical space, where distances between instances delineate their similarity~\cite{ito2018effects}. 
In this section, we discuss different RL techniques for both unimodal and multimodal datasets. 

\subsection{Learning representation from unimodal data}
Although the development of unimodal representations have widely studied, there has been a shift to multimodal data-driven~\cite{mmsurvey}. Deep belief networks~(DBN) is used as feature extractor in which restricted Boltzmann machines~(RBM)~\cite{jaitly2011learning} formed the basic building block. Each successive layer of a deep RBM is stacked together to represent the data at a higher level of abstraction. One advantage of RBM for RL is it does not need much labeled data for the training. Being the graphical models, the representation of data is probabilistic. Since the scale invariant feature transform was often used in imaging, which is the reason RBM and DBN were gradually shifted to CNN with which most visual descriptions are learned from data. 

\hspace*{3.5mm} Subsequently, different neural network architectures are proposed to improve the quality of the RL~\cite{min2018survey} from multimodal datasets. For example, visual representations based on CNNs and acoustic representations using epectrograms or with autoencoders, are among a few examples where the network learn unimodal features~\cite{ito2018effects}.
Despite of numerous successes of DBN and CNN, RL techniques have gradually been replaced with different variants of AE. In particular, convolutional autoencoders~(CAE), variational autoencoders~(VAE), adversarial autoencoders~(AAE), and LSTM-AE are used depending upon the types, structure, and nature of data,~(e.g., imaging, sequence) for the RL. 

\subsubsection{CAE}
Since a vanilla AE is not suitable for handling data with spatial invariance~(e.g., high resolution images), they are incapable of preserving spatial relationships between pixels in an object. However, CNN can be a better feature extractor as it can preserve local structure in which output from the deepest convolutional layer can be extracted as LF. In contrast, instead of manually engineered convolutional filters, convolutional and pooling layers can be added to construct a CAE, where each layer consists of an encoder~(that performs convolution and pooling operations), and a decoder~(to perform unpooling and deconvolution operations), and a convolutional layer of the encoder calculates the $j^{th}$ feature map as follows~\cite{alirezaie2019semantic}:

\begin{equation}
    h^{j}=\sigma\left(x_{i} * W_{ij}^{j}+b^{j}\right),
\end{equation}

\hspace*{3.5mm} where $x_i$ is the input sample, $W_{ij}^{j}$ is the $j^{th}$ filter from input channel $i$ and filter $j$, $b^j$ is the bias for the $j^{th}$ filter, i.e., single bias per latent map~(one bias per GV would introduce many degrees of freedom), $\sigma$ is an activation function, i.e., rectified linear unit~(ReLu), and $*$ denotes the convolutional operation. To obtain translation-invariant representations, max-pooling is performed by downsampling convolutional layer's output and taking the maximum value in each $m \times n$ non-overlapping sub-region~\cite{alirezaie2019semantic}. In the decoding phase, unpooling and deconvolution operations are performed to preserve the positional-invariance information. Deconvolution operation is then performed to reconstruct $x_i$ as~\cite{alirezaie2019semantic}:

\begin{equation}
   x_i = \sigma\left(o^{j} * W_{oj}^{j}+c^{j}\right),
\end{equation}

\hspace*{3.5mm} where $o^j$ is $j^{th}$ feature map and $W_{oj}^{j}$ is $j^{th}$ filter of unpooling layer $o$; $j$ and $c^j$ are filter and bias of $j^{th}$ output layer, respectively. Compared to CNN, CAE learns optimal filters and minimize the reconstruction loss, which results in more abstract features from the encoder~(e.g., pixel-level features from images) that help to stabilize training and network converges faster, avoid corruption in feature space~\cite{guo2017deep}. 

\begin{sidewaystable*}
   \caption{Comparison of autoencoders-based representation learning approaches~\cite{karimBIB2019}} 
   \label{tab:fe}
   \scriptsize % text size of table content
   \centering % center the table
   \begin{tabular}{p{3.5cm}|p{10.3cm}|p{10cm}}
    \hline
   \textbf{Feature extractor} & \textbf{Advantages} & \textbf{Disadvantages}\\ 
   \midrule
    \textbf{AE} & One of the simplest and MLP-based auto-encoding techniques. It learns hidden features to encode and decode the data without considering the probability distribution of the input samples. Hence it is easy to implement and extract features from the encoder component. & AEs have a huge number of hyperparameters, which is why it is tricky to optimize and balancing between clustering and non-clustering losses. Since it learns the hidden representation discriminatively to encode and decode the data blindly using a shallow network architecture. A fundamental problem with an AE is with the LF it embeds their inputs to and where their encoded vectors lie), may not be continuous and may allow easy interpolation. Consequently, CQ would be poor in the case of bioimaging and biological sequence data. Although the computational complexity depends on the clustering loss, it requires many iterations to optimize a large number of hyperparameters.\\\hline
    \textbf{DBN} & A simple generative model based on RBM, which has very rich mathematical and conceptual justification in its structure as well as its training algorithms. Works moderately well even in a limited labeled data set because it can be pre-trained in an unsupervised way, and the pre-training weights can be reused for a supervised learning task. & DBN-based RL has a risk of obtaining a corrupted LF space if the RBM pretraining loss goes out-of-bounds. Further, to avoid overfitting, it typically requires many samples to train well.\\\hline
    \textbf{CNN} & Has a straightforward graceful objective hence can be extended to large-scale clustering tasks. Deep and quality features can easily be extracted for numerous bioinformatics use cases, e.g., bioimaging, text (i.e., sequence) clustering, and genomics. It has a fewer number of hyperparameters than a regular AE or VAE, which makes it easier to optimize the overall network. & Since there is a risk of obtaining a corrupted LF space, a well-defined clustering loss is required to balance between clustering and non-clustering losses, which is tricky. To avoid overfitting, CNN typically requires many samples to get trained well.\\\hline
    \textbf{CAE} & Has straightforward graceful objective hence can be extended to large-scale clustering tasks. Deep and quality features can be easily extracted for bioimaging and text clustering. Further, since in CAEs, weights are shared among all locations in the input, preserving locality, and reducing the number of parameters than regular AEs, VAEs, and CNNs~\cite{lintas2017artificial}. & Since there is a risk of obtaining a corrupted LF space, a well-defined clustering loss is required to balance between clustering and non-clustering losses, which is tricky. Similar to CNN, CAE also requires many samples to be trained well to avoid overfitting.\\\hline
    \textbf{VAE} & Capable to generate artificial samples, which makes it suitable for bioinformatics use cases with limited labeled or unlabeled samples. Particularly suitable for numeric and genomic data. Besides, it has a decent theoretical guarantee and mathematical interpretation. & The computational complexity is very high, hence requires many iterations to optimize numerous hyperparameters. Exhibits poor clustering in the case of HQ bioimaging. \\\hline
    \textbf{AAE} & Capable to generate artificial samples, which makes it suitable for bioinformatics use cases with limited labeled or unlabeled samples. Particularly suitable for numeric and genomic data. Besides, the flexible nature of GAN and its variants can be used to disentangle both discrete and continuous latent factors. Hence, it can scale to complex datasets. & Since AAE's optimizing objective comprises several losses~(i.e., reconstruction loss, GMM likelihood, and adversarial objective), computation complexity is very high and hard to converge. \\\hline
   \end{tabular}
\end{sidewaystable*}

\subsubsection{Variational autoencoders} 
Generative variants of AE called variational autoencoders~(VAE) are used in literature in combination with a mixture of Gaussian. VAE enforces the latent code of AE to follow a predefined distribution, which combines variational Bayesian methods and increases the flexibility of the base network. Architecturally, VAE is different compared to AE or CAE and deeply rooted in the methods of variational Bayesian and graphical models, where the input is into distribution, as shown in \cref{fig:vae}. This distribution, say $p_{\theta}$, is parameterized by $\theta$, where $p_{\theta}(\mathbf{z})$ is the prior, $p_{\theta}(\mathbf{x} | \mathbf{z})$ is the likelihood, and $p_{\theta}(\mathbf{z}|\mathbf{x})$ is the posterior given that the real parameter $\theta^*$ is known for the distribution. 

\begin{figure*}[h]
	\centering
	\includegraphics[scale=0.5]{images/vae.png}	
	\caption[Schematic representation of a variational autoencoder]{Schematic representation of a VAE used for clustering GE data, where an individual GE sample is fed into the model for learning representation~\cite{karimBIB2019}}	
	\label{fig:vae}
	\vspace{-4mm} 
\end{figure*}

\hspace*{3.5mm} To generate a sample similar to a real data point $\mathbf{x}^{(i)}$: i) first, $\mathbf{z}^{(i)}$ can be sampled from a prior distribution $p_{\theta^{*}}(\mathbf{z})$, ii) then,  $\mathbf{x}^{(i)}$ can be generated from the conditional distribution $p_{\theta^{*}}\left(\mathbf{x} | \mathbf{z}=\mathbf{z}^{(i)}\right)$, $\theta^{*}$ is the optimal parameter that maximizes the probability of generating real data samples~\cite{VADE}:

\begin{equation}
    \theta^{*}=\arg \max _{\theta} \sum_{i=1}^{n} \log p_{\theta}\left(\mathbf{x}^{(i)}\right).
\end{equation}

\begin{equation}
    p_{\theta}\left(\mathbf{x}^{(i)}\right)=\int p_{\theta}\left(\mathbf{x}^{(i)} | \mathbf{z}\right) p_{\theta}(\mathbf{z}) d \mathbf{z}.
    \label{eq:data}
\end{equation}

\hspace*{3.5mm} The data generation process involving the encoding vector can be expressed in \cref{eq:data}~\cite{VADE}. Eventually, VAE consists of a probabilistic encoder as an approximation function $q_{\theta}(\mathbf{z} | \mathbf{x})$~(which is similar to $g_{\phi}(\mathbf{z} | \mathbf{x})$) and a generative probabilistic decoder as the conditional probability $p_{\theta}(\mathbf{x}|\mathbf{z})$~(which is similar to the decoder $f_{\theta}(\mathbf{x}|\mathbf{z})$). In variational inference, objective is to maximize the variational evidence lower bound (ELBO) by maximizing the lower bound (based on the fact that KL divergence is always non-negative, hence $-L_{vae}$ is the lower bound of $\log p_{\theta}(\mathbf{x})$) as follows~\cite{VADE}: 

\begin{equation}
    -L_{vae}=\log p_{\theta}(\mathbf{x})-L_{\mathrm{KLD}}\left(q_{\phi}(\mathbf{z}|\mathbf{x}) \| p_{\theta}(\mathbf{z}|\mathbf{x})\right) \leq \log p_{\theta}(\mathbf{x}). 
    \label{eq:lvae}
\end{equation}

\hspace*{3.5mm} VAE and its variants like LSTM-VAE~\cite{park2018multimodal}) are widely used across use cases,e.g., anomaly detection in which anomalous or outliers can be identified based on the reconstruction probability~(RP)~\cite{an2015variational}. RP is a probabilistic measure that takes into account the variability of the distribution of variables as: i) it has a theoretical background, and ii) more principled and objective anomaly score than the reconstruction loss. 

\subsubsection{LSTM-AE}
VAE or CAE are not the best options for handling sequence or time-series data, e.g., length of the input sequence may vary while the network requires fixed-length inputs. Further, the temporal ordering of the observations makes the feature extraction difficult. Hence, regular AEs will fail to generate a sample sequence for a given input distribution in generative mode, whereas LSTM-AE can handle variable lengths as the encoder learns fixed-length vector representation of the input~\cite{KarimNCCA2019,karim2019drug}. Given $X$ = $\{\mathbf{x}^{(1)},\mathbf{x}^{(2)}, ..., \mathbf{L}^{(1)}\}$ a input sequence, $\mathbf{h}_E^{(i)} \in {R}^c$ is encoder's hidden state at time $t_i$ for each $i \in \{1,2,...,L\}$, and $c$ is the number of LSTM units~\cite{LSTM_Autoencoder}. The encoder and decoder are jointly trained to reconstruct the original vector in reverse order by minimizing the following objective~\cite{zhu2018hidden}:  

\begin{figure*}[h]
	\centering
	\includegraphics[scale=0.5]{images/lstm_ae_v2.PNG}
    \caption[Schematic representation of the LSTM-autoencoder]{Schematic representation of the LSTM-AE, used for biomedical text clustering, where individual drug review texts are embedded using word2vec before feeding as a sequence~\cite{karimBIB2019}}	
	\label{fig:lstm_ae}
	\vspace{-2mm} 
\end{figure*}

\begin{equation}
    \sum_{X \in S_n}\sum_{i=1}^{L}\|\mathbf{x}^{(i)}-\mathbf{x}'^{(i)}\|^2
    \label{eq:obj}
\end{equation}

\hspace*{3.5mm} where $S_n$ is set of training sequences. The final state $\mathbf{h}_E^{(L)}$ of the encoder is used as the initial state for the decoder. The decoder uses $\mathbf{x}^{(i)}$ as input to obtain state $\mathbf{h}_D^{(i-1)}$ and predict $\mathbf{x'}^{(i-1)}$ corresponding to target $\mathbf{x}^{(i-1)}$~\cite{LSTM_Autoencoder} as shown in \cref{fig:lstm_ae}.  

\subsubsection{Adversarial autoencoders} 
In more recent approaches, adversarial autoencoders~(AAE) is employed in which the adversarial training procedure is followed to match the aggregated posterior of the latent representation with the prior distribution. AAE can be used to generate artificial samples with a limited number of labeled or unlabeled~(i.e., numeric or genomic) data, where the flexible nature of GAN can be utilized to extract discrete and continuous LF from large-scale data~\cite{min2018survey}. In particular, information maximizing generative adversarial network~(aka. InfoGAN)~\cite{chen2016infogan} is used for optimizing the mutual information between a fixed small subset of the GAN's noise variables and the observation~\cite{mcdaid2011normalized}, assuming: i) computation complexity is not a prime concern, ii) appropriate hyperparameters can be found. 

%\paragraph{\textbf{Sparse AE:}}  Besides, restrictions can be imposed to enforce the encoder to extract most salient features, e.g., sparsity constraint to obtain a sparse representation~\cite{min2018survey}. 
\iffalse
\subsubsection{Stacked autoencoders} 
The input can be denoised and passed through by stacking autoencoders~(SAE), e.g., where the input corruption is used only for the initial denoising. Once the mapping function $f_{\theta}$ is learned, the uncorrupted input from the previous layers are reused in the subsequent layers, e.g., then the weights of the network can be initialized with SDAE, where each layer is a DAE trained to reconstruct previous layer's output after random corruption~(i.e., DAE). SDAE can be considered a 2-layer network formulated as follows~\cite{xie2016unsupervised}:

\vspace{-6mm}
\begin{align}
    \begin{aligned}
        \tilde{x} & \sim d \text {ropout }(x) \\
        h &=g_{1}\left(W_{1} \tilde{x}+b_{1}\right) \\
        \tilde{h} & \sim d r o p o u t(h) \\
        y=& g_{2}\left(W_{2} \tilde{h}+b_{2}\right)
    \end{aligned}
\end{align}

\noindent where $dropout(.)$ is the dropout operation~\cite{srivastava2014dropout}, $g_1$ and $g_2$ are activation functions for encoding and decoding layer respectively, and $\theta$=$\lbrace{W_1, b_1, W_2, b_2}\rbrace$ are model hyperparameters~\cite{xie2016unsupervised}. Then greedy layer-wise training~(GLW) is performed by minimizing the least-squares loss $||x-y||^{2}_{2}$, i.e., after training one layer, output $h$ is used as the input to the next layer and so on. In such a scenario, ReLU activation function is used in all encoder and decoder pairs, except for $g_2$~(first pair) and $g_1$~(last pair). Once the GLW training is finished, all the encoder and decoder layers are concatenated in reverse layer-wise training order, by forming a deep AE and fine-tuned to minimize the reconstruction loss. During the GLW pre-training, each layer is pretrained for a relatively higher number of iterations with a dropout. The result is a multilayer deep AE with a bottleneck-coding layer in the middle. Based on a similar principle, other types of AE can be stacked to form such a deep AE architecture. 
\fi 

\subsection{Learning representation from multimodal data}
A multimodal representation is a representation of data using information from multiple entities~\cite{mmsurvey}. If the decision support~(or recommendation) provided by a decision support system relies on different types of input, such dataset can be characterized as multimodal. The ability to represent multimodal data in an efficient way forms the backbone of any predictive model. However, how to combine different types of data from heterogeneous sources, how to deal with different levels of noise and artifacts, and how to deal with missing data are few challenges in the multimodal RL. 

\iffalse
\vspace{-2mm}
\begin{tcolorbox}[colback=white!3!white,colframe=gray!120!black,title=\faBook~Input modality]
    %INFO: \faBook \\
    \scriptsize{
        \textbf{Input modality:} a decision support systems, which provides decision (e.g., clinical) relies on different types of data and in terms of dataset is characterized as multimodal. The term `modality' refers to the way in which we experience a real-life events that includes multiple such modalities~\cite{mmsurvey}.
        }
\end{tcolorbox}
\fi 

\begin{figure*}[htp!]
	\centering
	\begin{subfigure}{.48\linewidth}
		\centering
		\includegraphics[scale=0.4]{images/rl_1.png}
		\caption{Latent representation concatenation}
        \label{fig:lrc_1}
	\end{subfigure}
	\begin{subfigure}{0.48\linewidth}
		\centering
		\includegraphics[scale=0.4]{images/shared.png}
		\caption{Shared latent representation}
        \label{fig:slr_1}
	\end{subfigure}
	\caption[Fusion architectures in multimodal representation learning]{Fusion architectures in multimodal representation learning (only encoder module is shown)} 
	\label{fig:mm_rL_example}
\end{figure*}

\hspace*{3.5mm} Depending upon requirements, we may need one~(\textit{unimodal}) or multiple types of data~(i.e., \textit{multimodal}) such as genomics, proteomics, or imaging data into the neural networks. Although several works have focused on unimodal RL, recent approaches are more focused on multimodal representations from different types of data involving simple or shared concatenation of unimodal ones~\cite{mmsurvey}. \Cref{fig:mm_rL_example} shows two fusion architectures in multimodal RL. The latent representation concatenation architecture is depicted in \cref{fig:lrc_1} consists of learning simultaneously a single latent representation from each modality. In the subsequent chapters, we'll perform modality-specific latent representations to train a classifier. 

\begin{figure*}[htp!]
	\centering
	\begin{subfigure}{.48\linewidth}
		\centering
		\includegraphics[width=\linewidth,height=65mm]{images/bagging.png}
		\caption{Bagging}
        \label{fig:bagging}
	\end{subfigure}
	\hspace{2mm}
	\begin{subfigure}{0.48\linewidth}
		\centering
		\includegraphics[width=\linewidth,height=65mm]{images/stacking.png}
		\caption{Stacking}
        \label{fig:stacking}
	\end{subfigure}
	\caption[Bagging vs. stacking]{Bagging vs. stacking: a) each model is individually trained and the predictions are voted upon, b) multiple weak learners are summed upon} 
	\label{fig:bagging_and_stacking}
	\vspace{-2mm}
\end{figure*}

\section{Model Ensemble Methods}
The ensemble modelling is based on a principle in which weak learners is grouped together to form one strong learner. First, multiple models are trained, each with the objective to predict or classify a set of results. By using ensemble methods, the stability of the final model can be increased by reducing the errors. In particular, by combining many models, mostly the variance error can be reduced. For example, by maximum voting from a panel of independent radiologists, we get the final prediction fair and trustworthy than a single radiologist. 
Model ensembling helps to achieve improved performance of a model compared to the predictions from a single model by reducing the generalization error~\cite{karimACCA2019}. Bagging, boosting, and stacking are three widely used concepts in ensembling learning to improve the classification accuracy by reducing model variance, noise, and bias: 

\begin{itemize}[noitemsep]
    \item \textbf{Bagging} is about bootstrapping and aggregating the scores over many samples. It is an effective method in case of limited labelled data. In a limited labelled data setting, smaller training set is generated by re-sampling the original training set having the sample cardinality. Bootstrapping reduces the probability of overfitting, which decreases the variance in the ensemble model. 
    \item \textbf{Boosting} is adding additional models to the ensemble model sequentially to decrease model’s bias.
    \item \textbf{Stacking} is a an ensemble method in which a new model is trained from the combined predictions of two or more previously trained models, where predictions from the previously trained models are used as inputs for each sequential layer, and combined to form a new set of predictions. Stacking helps to increase the predictive power of the final model. 
\end{itemize}

\hspace*{3.5mm} When it comes to neural ensemble, it can be achieved by training multiple model snapshots during a single training of a neural network and combining their predictions to make an ensemble prediction called snapshot ensemble~\cite{huang2017snapshot}. A limitation of this approach, however, might be that the saved models will be similar, resulting in similar predictions and predictions errors. Hence, we cannot expect much benefit from combining their predictions unless we have already introduced diversities during model training~\cite{huang2017snapshot}. This issue is addressed in \cref{chapter:uni_modality}, by changing the learning algorithm to force the exploration of different network weights during a single training run that will result slightly different performance\cite{karimACCA2019}. 

\section{Hyperparamters Tuning}
ML models are composed of two different types of parameters called model parameters and hyperparameters. While the former parameters are learned during the model training such as weights in neural networks, the latter parameters can be arbitrarily set by the user before starting training like number of estimators in random forest or learning rate in a neural network. In other words, model parameters are learned during the training, while hyperparameters are set by the human, e.g., data scientist before the training.

\hspace*{3.5mm} Often parameters that define model architecture are called hyperparameters, where the prefix `hyper refers that it is not a `model parameter', i.e., can be optimized during the training. More technically, model parameters are learned during the training while we optimize a loss function using an optimizer such as RMSProp, whereas hyperparameters are not model parameters and cannot be directly trained from the data. Following are a few examples of hyperparameters: 

\vspace{-1mm}
\begin{itemize}[noitemsep]
    \item \textbf{Linear model} - degree of polynomial features.
    \item \textbf{Clustering} - number of clusters~(i.e., k) in a k-means clustering.
    \item \textbf{Tree and tree-ensemble} - maximum depth of the decision tree~(DT), the minimum number of samples at a leaf node in the DT, number of trees to include in a random forest model. 
    \item \textbf{Neural networks} - number of neurons in a hidden layer, number of layers, learning rate for stochastic gradient descent for backpropagation. 
\end{itemize} 
\vspace{-2mm}

\iffalse
\begin{tcolorbox}[colback=white!3!white,colframe=gray!120!black,title=\faBook~Random vs. grid search for hyperparameter optimization]
    \scriptsize{
        \textbf{Grid search:} in the following figure, $x$ number of values are picked, which are evenly spaced along each axis, forming a grid. Then a model is trained for each possible combination, followed by evaluating each model, and selecting the architecture giving the best result. \\
        }
    \includegraphics[width=0.8\textwidth]{images/gvr.png}\\ 
    \scriptsize{
        \textbf{Random search:} $x$-squared number of values are picked randomly and used in cross validation to test the accuracy of the experiment for each combination.
        } 
\end{tcolorbox}
\fi 

\subsection{Techniques for hyperparameter optimization}
Since an appropriate selection of hyperparameters have huge impact on the performance of ML model, `hyperparameter tuning' is performed, which is the process of searching best model based on optimal combination of hyperparameters. In other words, hyperparameter optimization~(HPO) is the problem of choosing the best combination~(or optimal) hyperparameters for a learning algorithm. HPO is often represented as the following objective function: 

\vspace{-6mm}
\begin{align}
    x^{\star}=\arg \min _{x \in \mathcal{X}} f(x)
    \label{eq:hpt}
\end{align}

\hspace*{3.5mm} where $x \in X$, $f(x)$ represents an objective score to minimize, e.g. RMSE evaluated on the validation set and $x^*$ is the set of hyperparameters to yield the lowest value of the score. However, in case of complex model, there are more than one hyperparameter on which there are several approaches to perform hyperparameter tuning such as grid search, random search, and Bayesian optimization. 

\begin{figure*}[h]
	\centering
	\includegraphics[width=0.8\textwidth,height=50mm]{images/gvr.png}
    \caption{Random vs. grid search for hyperparameter optimization}	
	\label{fig:random_vs_grid_search}
	\vspace{-2mm} 
\end{figure*}

\subsection{Grid search and random search}
In grid search, evenly spaced values are generated for each hyperparameter being tested, followed by building a model for each possible combination of all of the hyperparameter values, evaluating each model and selecting the architecture which produces the best results. %\subsection{Random search}
In random search, random values for each hyperparameter are generated that are used in cross-validation to evaluate the model for each combination.

\begin{itemize}[noitemsep]
    \item In \textbf{grid search}, $x$ number of values are picked, which are evenly spaced along each axis, forming a grid, as shown in \cref{fig:random_vs_grid_search}. Then a model is trained for each possible combination, followed by evaluating each model, and selecting the architecture giving the best result.
    \item In \textbf{random search}, $x$-squared number of values are picked randomly and used in cross validation to test the accuracy of the experiment for each combination, as shown in \cref{fig:random_vs_grid_search}.
\end{itemize}

\hspace*{3.5mm} Random search is better than grid search for most cases, hyperparameters are not equally important, making some hyperparamters are more important than others. A Gaussian process analysis of the function from hyper-parameters to validation set performance reveals that for most data sets only a few of the hyper-parameters really matter, but that different hyper-parameters are important on different data sets. This phenomenon makes grid search a poor choice for configuring algorithms for new data sets. %- Bergstra, 2012

\subsection{Bayesian optimization} 
Both grid search and random search perform individual experiments~(i.e., one model training) while training the model with various hyperparameter values and observe the performance of the model in each case. Many optimization problems in ML are black box optimization problems, where the objective function $f(x)$ in \cref{eq:hpt} is a black box function~\cite{BO}. It neither interprets the analytical expression for $f$, nor we know its derivatives. Oftentimes, evaluation of the function is restricted to sampling at a data point $x$ and getting a possibly noisy response~\cite{BO}.
Nevertheless, as each experiment using grid search and random search is performed separately, reusing the result of current experiment to improve the next one is not possible. However, they can easily be parallelized by employing an approach called ``Bayesian optimization"~(BO), which belongs to a class of sequential model-based optimization algorithms. BO allows using the results of the previous iteration to improve the sampling method of the next experiment.

\hspace*{3.5mm} BO incorporates prior belief about $f(x)$ and updates the prior with samples drawn from $f(x)$ to get a posterior that better approximates $f(x)$. The model used for approximating the objective function is called surrogate model. In BO, a model is initially defined and trained with hyperparameters $\sigma$, which is scored $v$ w.r.t some evaluation metrics\footnote{Metrics could be different depending upon task at hand}. The previously evaluated hyperparameter values are reused to compute a posterior expectation of the hyperparameter space. Optimal hyperparameter values are then chosen according to this posterior expectation as the next model candidate, which is repeated iteratively until it converges to the optimal value. While, the Gaussian process is employed to model the prior probability of model scores across the hyperparameter space,  
%This model then enables to use the hyperparameter values $\sigma_1,\sigma_2,...,\sigma_n$ and corresponding scores $v_1,v_2,...,v_n$ to approximate a continuous score function over the hyperparameter space. %It includes the degree of certainty of the estimate, which can be used to identify the candidate hyperparameter values, which hopefully would yield the largest expected improvement over the current score. 
BO finds a better optimum in a smaller number of steps than random search and beats the baseline in almost every run~\cite{BO}. Using BO in case of a clinical DSS becomes even more evident, given the fact that we have such a high-dimensional search spaces, e.g., many omics data often have 20K+ dimensionality. 

\section{Interpretability of ML Models}
Although DL models have shown tremendous success at AI-aided diagnosis and prognosis, showing high effectiveness and accuracy, there are many cases where not only high accuracy, but also explanation about the outcome is necessary. Suppose we have an accurate model that can classify cancerous samples with an accuracy of 90\%. Still, we cannot claim that it has high confidence at providing accurate and trustworthy diagnosis, given that the single accuracy metric is an incomplete description~\cite{doshi2017towards} and can't be enough in AI-aided diagnosis and prognosis, unless it explains the reason behind the  decision. Otherwise, such a model has to perceive a `black box' method as it cannot explain why it reached to such a diagnosis decision. % and lack of transparency.  
If the parameters $\boldsymbol{\theta}$ of a ML model $\boldsymbol{f}$ and it's architecture are known, the model is considered a `white-box'. %A white-box model also helps improve model-debugging and promotes trust. 
A `white-box' model is not necessarily explainable or interpretable~\cite{das2020opportunities}, while an ML model $\boldsymbol{f}$ is considered a `black-box' if the model parameters and network architectures are hidden from the end-user. We slightly tweaked the definitions of `black-box' and `white-box' models provided by Das et al.~\cite{das2020opportunities}: 

\begin{definition}
    \textbf{black-box model} - an ML model $f$ is considered `black-box' if the model parameters and network architectures are hidden from the end-users. 
\end{definition}

\begin{definition}
    \textbf{white-box model} - an ML model $f$ is `white-box' if its parameters $\boldsymbol{\theta}$ and architecture are known to the end users, by providing sufficient knowing of the working principles. % and why it provides a specific decision.  
\end{definition}

\hspace*{3.5mm} In contrast, interpretable ML refers to methods that make the behavior and predictions of a system understandable to humans~\cite{molnar2019interpretable}. 
Interpretability of an ML system is the ability to explain or to present in understandable terms to a human~\cite{doshi2017towards}. Miller et al.~\cite{XAI_miller} defines interpretability as the degree to which a human can understand the cause of a decision. Although a white-box model improves model-debugging and promotes trust, knowing it's architecture and parameters alone won't make the model essentially explainable~\cite{das2020opportunities}.  
Model interpretation is an extension of the model evaluation, which helps to foster a better understanding of a model’s learned decision policies and gives the ability to explain a model in a way which is human understandable such that the outcomes are self-descriptive and needs no further explanation. 
The higher the interpretability, the easier it is for human to comprehend why a particular decision or prediction is made~\cite{stiglic2020interpretability,bhatt2020explainable}, i.e., trustworthiness for the human. In either way, good interpretability is desired both locally, e.g., individual predictions and globally, e.g., entire model behavior. 

\begin{figure*}[h]
	\centering
	\includegraphics[scale=0.55]{images/xai_tec.png}	
    \caption[Categorization of XAI techniques]{Categorization of XAI techniques in terms of scope, methodology, and usage~(based on~\cite{das2020opportunities})}	
	\label{fig:survey_xai}
\end{figure*}

\hspace*{3.5mm} Although many authors used interpretability and explainability interchangeably, the former means that the cause and effect~(e.g., of a certain outcome) can be determined in a system. The latter is the extent to which the internal working mechanism of an AI system can be explained to humans, in human-interpretable language. Miller et al.~\cite{miller2018explanation} defined explanation as the answer to `why' and `how' types of questions. For example, in the cancer diagnosis context, a patient may have the following questions: i) why do I have colon cancer?, ii) Why did the treatment not work on me?, iii) how did the model predict the decision?, etc. However, there is a trade-off between explianability and accuracy. How well does an explanation predict unseen data is a measurement of accuracy~\cite{molnar2019interpretable}. High accuracy is important if the explanation is used for predictions in place of the ML model. However, if the goal is to explain what the `black-box' model does~(i.e., algorithmic transparency), only `fidelity' is important~\cite{molnar2019interpretable}. 

\subsection{Algorithmic transparency}
An ML model is considered transparent if it is expressive enough to be human understandable, where transparency can be a part of the algorithm itself or using external means~(e.g., model decomposition or simulations)~\cite{das2020opportunities}. Although the phrase `algorithmic transparency' was coined by Nicholas D. et al.~\cite{diakopoulos2017algorithmic} in 2016 w.r.t the role of algorithms in deciding the content of digital journalism, the underlying principle dates back to 1970s and the rise of automated systems for scoring consumer credit~\cite{diakopoulos2015algorithmic}.  Algorithmic transparency suggests \textit{factors that influence the decisions made by algorithms should be visible, or transparent, to the people who use, regulate, and are affected by systems that employ those algorithms}~\cite{diakopoulos2017algorithmic}.  %\footnote{\url{https://en.wikipedia.org/wiki/Algorithmic_transparency}}. 

\iffalse
\begin{definition}
    {\textbf{algorithmic transparency}} is the principle that the factors that influence the decisions made by algorithms should be visible, or transparent, to the people who use, regulate, and are affected by systems that employ those algorithms.
\end{definition}
\fi 

\hspace*{3.5mm}More technically, ``algorithmic transparency" is how the algorithm learns a model from the data and maps different relationships transparently based on what prediction it makes for unknown samples. First example could be a decision tree algorithm, which implicitly performs variable screening or feature selection. Hence, feature importance is clear, showing clear interactions between features. The second example could be image classification using neural networks, e.g., while classifying images, a CNN model learns features~(e.g., class-discriminating pixels) using edge detectors and filters across convolutional layers. Subsequently, it can learn more abstract features in the deepest layer to provide attention on some areas using attention maps, explaining where most important pixels are located.

\iffalse
\vspace{-2mm}
\begin{tcolorbox}[colback=white!3!white,colframe=gray!120!black,title=\faBook~Algorithmic transparency]
    %INFO: \faBook \\
    \scriptsize{
        \textbf{Algorithmic transparency:} is how a learning algorithm learns a model from the data by mapping relations to make prediction for unknown sample.
        }
\end{tcolorbox}
\fi 

\hspace*{3.5mm} 
%Less technically, Algorithmic transparency is how a learning algorithm learns a model from the data by mapping relations to make prediction for unknown sample. 
There are a several ways to transparently show how a learning algorithm works. Some models are inherently interpretable, some are not. For example, DNNs that are propagating gradients through a network with millions of weights are less understood because of their inner workings, are less transparent~\cite{molnar2019interpretable}. However, they way a linear~(linear or logistic regression) or tree-based model~(decision trees) learn is very transparent and is characterized by a high transparency. Algorithmic transparency requires only knowledge of the algorithm, not of the data or learned model itself~\cite{molnar2019interpretable}. Implicitly, ``algorithmic transparency" also means that the inputs used by the algorithm must be known, but they necessarily need not be fair. 

\subsection{Needs for interpretability}
Although, not all predictions made by an ML algorithm needs to be explained, interpretability is essential for end users's trust and to effectively manage the emerging generation of AI applications~\cite{das2020opportunities}. In general, ML models needs to be interpretable, higher interpretability of the model means easier comprehension and explanation of future predictions for end-users~\cite{stiglic2020interpretability}. With the wider usage of AI in numerous domains, the need to explain an ML model result is important for ethical, judicial, as well as safety reasons~\cite{das2020opportunities}. 
%Literature have discussed important aspects showing the importance of the XAI. Among others, 
Three most important concerns identified by Das et al.~\cite{das2020opportunities} are trustability, transparency, and fairness of AI algorithms. \Cref{fig:need_for_xai} depicts some critical areas where both explainability and transparency are crucial. 
%Current business models include interpretation as a step before serving the ML models on production systems, however are often limited to small tree-based models. With the use of highly nonlinear deep learning algorithms with millions of parameters in ML pipelines, XAI techniques must improve all three concerns mentioned above. 
%However, interpretability stems from an incompleteness in the problem formalization, which evolves a fundamental barrier to optimization and evaluation~\cite{doshi2017towards}. Note that incompleteness is distinct from uncertainty: the fused estimate of a missile location may be uncertain, but such uncertainty can be rigorously quantified and  formally reasoned about. In ML terms, we distinguish between cases where unknowns result in quantified variance—e.g. trying to learn from small data set or with limited sensors—and incompleteness that produces some kind of unquantified bias—e.g. the effect of including domain knowledge in a model selection process. In short, interpretability is desired and essentials in several aspects, e.g., scientific understanding and ethics.

\iffalse
\begin{figure*}[h]
	\centering
	\includegraphics[width=\textwidth,height=40mm]{images/adv_xai.png}	
    \caption{Significant expected improvements when using XAI techniques to support decision making of end-users. We believe XAI is important due to improvements in trust, transparency, and in understanding bias and fairness.~(based on~\cite{das2020opportunities})}	
	\label{fig:survey_xai}
\end{figure*}
\fi 

\begin{figure*}[h]
	\centering
	\includegraphics[scale=0.8]{images/why_xai.png}	
    \caption{Needs for explainability and transparency in DSS}	
	\label{fig:need_for_xai}
\end{figure*}

%\hspace*{3.5mm} XAI techniques improve the transparency by creating a human-understandable justification to the decisions and could find and deter adversarial examples if used properly~\cite{das2020opportunities}. XAI techniques can improve trust. Trustability of an ML model is a measure of confidence in the intended working of a given model in dynamic real-world environments. Often decisions and judgements are primarily based on the knowledge and available explanations to situations and the trust we generate. 
%A scientific explanation or logical reasoning for a sub-optimal decision is better than a highly confident decision without any explanations~\cite{das2020opportunities}. XAI techniques improve fairness by mitigating different types of bias. While bias in ML realm indicate the disproportionate weight, prejudice, favor, or inclination of the learnt model towards subsets of data due to both inherent biases in human data collection and deficiencies in the learning algorithm, fairness is the quality of a learned model in providing impartial decisions without favoring any populations in the input data distribution~\cite{das2020opportunities}.
%Eventually, XAI promotes fairness and helps mitigate biases introduced to the AI decision either from input datasets or poor neural network architecture.

\hspace*{3.5mm} Further, the article 14 of the EU GDPR~\cite{kaminski2019right} states that when a company uses automated decision-making tools, it must provide ``meaningful information about the logic involved, as well as the significance and the envisaged consequences of such processing for the data subject”~\cite{kaminski2019right}. In the same time, GDPR prohibits the use of ML for automated decisions unless a clear explanation of the logic used to make each decision is well-explained, which is not possible with conventional ML; mainly, due to the black-box nature. In case of ML-based profiling and automated decisions, following requirements have to be ensured~\cite{doshi2017towards}: 

\begin{enumerate}[noitemsep]
    \item Ability to identify processes involved in the business.
    \item Monitoring the ML model being currently used.
    \item Assessing the deployed model against the interpretability and prediction biases. 
    \item Assessing the ability of the model for generating human-interpretable decision rules. 
    \item Developing a strategy for meeting compliance requirements in each stage of the ML workflow.  
\end{enumerate}

\subsection{Scopes and implementation levels for interpretability}
\Cref{fig:survey_xai} shows the categorization of the survey in terms of scope, methodology, and usage. As shown, a wide range of strategies for interpretable ML have been developed and applied to numerous areas, ranging from model-agnostic to model-specific. They are often characterized based on if they provide global or local interpretations~\cite{azodi2020opening}. Depending on the abstraction, two types of interpretability called local and global interpretability can be achieved, to explain a single prediction or entire model's behaviour~\cite{molnar2019interpretable}: 

\begin{itemize}[noitemsep]
    \item \textbf{Global interpretability} explains how does a trained model make predictions~\cite{molnar2019interpretable}. Using global interpretation, it is possible to explain the conditional interaction between dependent and independent variables based on the training set, i.e., it involves explaining the overall relationship between features and labels, which gives a model the ability to explain it's entire behaviour~\cite{molnar2019interpretable}. 
    \item \textbf{Local interpretability} explains why did the model make a certain prediction for an instance~\cite{molnar2019interpretable}. Using local interpretation, it is possible to explain the conditional interaction between dependent and single prediction, i.e., focuses on explaining the prediction of an individual instance. Less technically, local explainability is the ability to explain individual predictions.  
\end{itemize}

\iffalse
\vspace{1mm}
\begin{tcolorbox}[colback=white!3!white,colframe=gray!120!black,title=\faBook~Local vs. global interpretation]
    %INFO: \faBook \\
    \scriptsize{
        \textbf{Global interpretability:} how does the trained model make predictions~\cite{molnar2019interpretable}? \\
        \textbf{Global interpretation:} being able to explain the conditional interaction between dependent and independent variables based on the training set, i.e., it involves explaining the overall relationship between features and labels.
        } \\ \\
    \includegraphics[width=0.6\textwidth,height=60mm]{images/lvg.png}\\ 
    \scriptsize{
        \textbf{Local interpretability:} why did the model make a certain prediction for an instance~\cite{molnar2019interpretable}? \\
         \textbf{Local interpretation:} being able to explain the conditional interaction between dependent and single prediction, i.e., focuses on explaining the prediction of an individual instance. 
        } 
\end{tcolorbox}
\fi 

%However, we can also classify such cases as either global if the subgroup is treated as the sub-population or as local if single prediction interpretations for the subgroup are grouped together~\cite{molnar2019interpretable}. 
\hspace*{3.5mm} A more concrete example related to the cancer diagnosis use case would be as follows: suppose, an ML model is trained to predict if a gene is up-regulated~(i.e., label) after some treatment based on the presence or absence of a set of regulatory sequences~(i.e., features). While a global interpretation strategy will tell you how important regulatory sequence $X_i$ is for predicting up-regulation across all genes in the dataset, a local interpretation strategy will tell you how important regulatory sequence $X_i$ is for predicting gene $y_i$ as up-regulated. As Azodi et al~\cite{azodi2020opening} suggest to emphasize on ML models identify association through correlation. This implicitly means that ML interpretation strategies do not necessarily meant to identify causal relationships between input features and labels. %\subsection{Model-specific vs. model-agnostic interpretability}

\begin{figure*}[h]
	\centering
	\includegraphics[scale=0.7]{images/lvg_cancer.png}	
    \caption[Scopes and types of ML models in healthcare]{Scopes and types of ML models in healthcare based on their interpretability characteristics~(inspired by scopes and types by Stiglic et al.~\cite{stiglic2020interpretability}), showing an example of cancer types prediction}	
	\label{fig:local_vs_global_ex}
\end{figure*}

%\hspace*{3.5mm} Local interpretability of an ML model can be achieved by designing justified model architectures that explains why a specific decision was made or by providing similar examples of instances to the target instance~\cite{stiglic2020interpretability}. For example, for our cancer diagnosis use case, by emphasizing specific characteristics of a patient that represents the characteristics of a smaller group of cancer patients such as breast cancer, yet different in other patients~(rather than all the patients in a dataset).   In contrast, global interpretability signifies the overall transparency of the model inside a model on an abstract level. Nevertheless, an interpretable ML can be developed that can have cohort-specific interpretability, where they focus on population subgroups~\cite{stiglic2020interpretability}. 

%The former is limited to specific models by which the explanations can be derived by examining internal model parameters~\cite{stiglic2020interpretability}. The latter is, however, applicable on any ML model in a post-hoc manner~\cite{molnar2019interpretable}. 

\iffalse
\begin{figure*}[h]
	\centering
	\includegraphics[width=\textwidth,height=70mm]{images/lvg_cancer.png}	
    \caption{Types of ML models in healthcare based on their interpretability characteristics (based on~\cite{stiglic2020interpretability}, showing an example of cancer types prediction}	
	\label{fig:local_vs_global_ex}
\end{figure*}
\vspace{2mm}
\begin{tcolorbox}[colback=white!3!white,colframe=gray!120!black,title=\faBook~Model surrogate]
    %INFO: \faBook \\
    \scriptsize{
        \textbf{Surrogate strategies:} model surrogation is a model interpretation strategies, which involve training an inherently interpretable model using the same data as a `black-box' model to approximate the predictions of the `black-box' model. }
\end{tcolorbox}
\fi 

\subsection{Techniques for interpretability}
Local interpretability of an ML model can be achieved by designing justified model architectures that explains why a specific decision was made or by providing similar examples of instances to the target instance~\cite{stiglic2020interpretability}. For example, for our cancer diagnosis use case, by emphasizing specific characteristics of a patient that represents the characteristics of a smaller group of cancer patients such as breast cancer, yet different in other patients~(rather than all the patients). In contrast, global interpretability signifies the overall transparency of the model inside a model on an abstract level. An interpretable ML can be developed that can have cohort-specific interpretability, where they focus on population subgroups~\cite{stiglic2020interpretability}. 

\hspace*{3.5mm} Based on outputs returned by the black-box model, \textit{surrogate} or a simple proxy model is often developed to learn a locally faithful approximation of a complex model~\cite{stiglic2020interpretability}. Model surrogation is a model interpretation strategy, which involve training an inherently interpretable model using the same data as a `black-box' model to approximate the predictions of the `black-box' model. Besides, probing and perturbing are also used as ML interpretation strategies, as shown in \cref{fig:pro_per_surroga}. Interpreting a model's outcome can be classified according to the results of the prediction model, as outlined by Molnar et al.~\cite{molnar2019interpretable}:

\vspace{-2mm}
\begin{itemize}[noitemsep]
    \item \textbf{Feature summary statistics} - summary of each feature and their impact on the model predictions. 
    \item \textbf{Feature summary visualization} - summary of the methods used to visualize and in order to make the visual communication easier, where outcomes are  presented with bars, plots, or table. 
    \item \textbf{Model internals approach} - the interpretation of intrinsically interpretable models, such as linear models in which model weights represent both internals parameters and summary statistics for the features. However, internal parameters of model-agnostic models are not typically inspected~\cite{molnar2019interpretable}. 
    \item \textbf{The data point interpretability} - methods that require data points themselves to allow interpretability to  return data points to make model interpretable. 
\end{itemize}

\hspace*{3.5mm} To explain and interpret the model predictions these ways, several approaches and methods have been proposed. 
\Cref{fig:xai_timeline} shows a timeline of XAI algorithms, covering scopes, methodology, and usage level~(based on~\cite{das2020opportunities}). In this subsection, we'll briefly cover most widely used ones. 

\subsubsection{Feature-based attribution methods}
\label{subsubsec:FI_shap}
For the human-level interpretability of a model, we need to know the inner insights, e.g., what features does a model think are most important and significant? Some of the features have higher impact than the others. This concept is called feature importance, which can be computed based on permutation importance~(PI). Intuitively, we can think of such importance both locally and globally, e.g., for a single prediction, we can compute the effect of each feature in the data, or, effect of each feature over a large number samples and overall the predictions.

\begin{sidewaysfigure*}[htp!]
	\centering
	\begin{subfigure}{.48\linewidth}
		\centering
		\includegraphics[scale=0.6]{images/probing.png}
		\caption{Probing}
        \label{fig:probing}
	\end{subfigure}
	\hspace{2mm}
	\begin{subfigure}{0.48\linewidth}
		\centering
		\includegraphics[scale=0.6]{images/purturbing.png}
		\caption{Stacking}
        \label{fig:purturbing}
	\end{subfigure}
		\begin{subfigure}{0.48\linewidth}
		\centering
		\includegraphics[scale=0.6]{images/surrogation.png}
		\caption{Stacking}
        \label{fig:surrogation}
	\end{subfigure}
	\caption{Different interpretability method for opening black-box ML model} 
	\label{fig:pro_per_surroga}
	\vspace{-2mm}
\end{sidewaysfigure*}

\iffalse
\begin{sidewaysfigure*}[h]
	\centering
	\includegraphics[scale=0.8]{images/pro_per_surrogation.png}
    \caption{Different interpretability method for opening black-box ML model}	
	\label{fig:pro_per_surroga}
\end{sidewaysfigure*}
\fi 

\hspace*{3.5mm} Feature importance methods define an explanation function $g: F \times \mathbb{R}^{d} \mapsto \mathbb{R}^{d}$ that takes in a model $F$ and a point of interest $x$ and returns importance scores $g(F,x) \in \mathbb{R}^{d}$ for all features is the importance or attribution for feature $x_i$ of sample $x$. In other words, PI works by randomly permuting or shuffling a single column in the validation dataset leaving all the other columns intact, where a feature is considered ``important” if and only if the model's accuracy drops significantly and the error is increased. On the other hand, a feature is considered ``unimportant’ if shuffling its values does not affect the model's accuracy significantly. Explanation functions roughly fall into two categories: i) perturbation-based techniques, ii) saliency map and gradient-based techniques. 

\begin{sidewaysfigure*}
	\centering
	\includegraphics[scale=0.6]{images/xai_roadmap.png}	
    \caption[Timeline of XAI methods and algorithms]{A timeline of XAI methods, covering scopes, methodology, and usage level~(based on survey of XAI techniques by Das et al.~\cite{das2020opportunities})}	
	\label{fig:xai_timeline}
\end{sidewaysfigure*}

\hspace*{3.5mm} A widely used perturbation based methods is based on Shapley values inspired by cooperative game theory. A Shapley Value is the average marginal contribution of a feature value over all possible combinations of features which are used to estimate the Shapley value of a specific feature, whereas Shapley values are a way to distribute the gains to its players. SHapley Additive exPlanations~(SHAP) is a unified approach to explain the output of any ML model. SHAP connects game theory with local explanations, uniting several previous methods and representing the only possible consistent and locally accurate additive feature attribution method based on expectations~\cite{NIPS2017_7062}. 

\subsubsection{Saliency map and gradient-based attribution methods}
The saliency map and gradient-based attribution methods are used to identify relevant regions and assign importance to each input feature, e.g., pixel for image data. Typically first-order gradient information of a complex model w.r.t is used to produce maps that indicate the relative importance of the different input features for the classification. Gradient-weighted class activation mapping~(Grad-CAM)~\cite{selvaraju2017grad}, it's improved variant called Grad-CAM++~\cite{chattopadhay2018grad}, sensitivity analysis~(SA)~\cite{baehrens2010explain, simonyan2013deep}, and layer-wise relevance propagation~(LRP) are examples of this category. 

\subsubsection{Sensitivity analysis}
SA explains a prediction based on the model's locally evaluated gradient, aka. partial derivatives~(PD). Mathematically, SA quantifies the importance of each input variable $i$~(at low level, e.g., image pixel) $R_{i}=\left\|\frac{\partial}{\partial x_{i}} f(\mathbf{x})\right\|$. This measure assumes that most relevant input features are those for which the output is sensitive. Usually, a heatmap~(HM) is plotted to visualize and indicate which pixels need to be changed to make the image look similar to the predicted class. However, such HM does not indicate which pixels are actually pivotal for a specific prediction. Consequently, SA is not suitable for quantitative evaluation. 

\subsubsection{Grad-CAM, Grad-CAM++, and Score-CAM}
Guided backpropagation~(GB)~\cite{springenberg2014striving} is another method, where absolute values of the gradient of the output with respect to the input nodes are shown as heatmap, with the additional twist that negative gradients are set to zero at the rectification layers of the network is based on the fact is ``rectifying” the gradients in the backward pass leads to more focused heatmap~\cite{bohle2019layer}. While class-discriminating attention map visualization are appended to exhibit significant features for class assignment. To overcome the opaqueness of CNN, class activation maps (CAM)~\cite{zhou2016learning} is proposed. 

\hspace*{3.5mm} To explain where the network provides more attention, CAM computes the weights w.r.t each feature map from the final convolutional layer. CAM calculates the contribution to prediction $y_c$ at location $(i,j)$. To obtain $L_{ij}^{c}$ that satisfies $y^{c}=\sum_{i, j} L_{ij}^{c}$. The last feature map $A_{ijk}$ and prediction $y_c$ are represented in a linear relationship in which linear layers consist  of global average pooling (GAP) and FCL: i) GAP outputs $F_{k}=\sum_{i,j} A_{ijk}$, ii) FCL that holds weight $w_{k}^{c}$, generates the following output~\cite{kim2020extending}: 
 
 \vspace{-4mm}
 \begin{align}
     y^{c}=\sum_{k} w_{k}^{c} F_{k}=\sum_{k} w_{k}^{c} \sum_{i, j} A_{i j k}=\sum_{i, j} \sum_{k} w_{k}^{c} A_{i j k}.
 \end{align}
 \vspace{-4mm}
 
\hspace*{3.5mm} where $L_{i j}^{c}=\sum_{k} w_{k}^{c} A_{i j k}$~\cite{kim2020extending}. Subsequently, heat maps~(HM) are then plotted to visualize the weighted combination of the feature maps. However, if linear layers replace the classifier architecture, re-training of the network is required and non-linearity of classifier vanishes. Subsequently, literature~\cite{114} came up with an efficient generalization of CAM called Grad-CAM, where instead of pooling, globally averages gradients of feature maps as weights, w.r.t, aiming at class $c$. The guided backpropagation in Grad-CAM helps to generate more human-interpretable but fewer class-sensitive visualizations than the saliency maps~(SM)~\cite{nie2018theoretical}. Since SM use true gradients, trained weights are likely to impose a stronger bias towards specific subsets of the input pixels. Accordingly, class-relevant pixels are highlighted rather than producing random noise~\cite{nie2018theoretical}. Therefore, Grad-CAM is used to draw the HM to provide attention to discriminating regions, where the class-specific weights of each FM are collected from the final convolutional layer through globally averaged gradients~(GAG) of FMs instead of pooling~\cite{chattopadhay2018grad}: 

\vspace{-4mm}
\begin{equation}
    \alpha_k^c=\frac{1}{Z}\sum_{i}\sum_{j}\frac{\partial y^c}{\partial A_{ij}^k}.
    \label{eq:alpha}
\end{equation}
\vspace{-4mm}

\hspace*{3.5mm} where $Z$ is number of pixels in a FM, $c$ is gradient of the class, and $A_{ij}^k$ is the value of $k^{th}$ FM. Having gathered relative weights, the coarse SM, $L^c$ is computed as the weighted sum of $\alpha_k^c*A_{ij}^k$ of the ReLU activation and employ the linear combination to the FM, as features with only positive influence on the class are of interest~\cite{chattopadhay2018grad} and negative pixels that belong to other categories in the image are discarded~\cite{114}:

\vspace{-4mm}
\begin{equation}
    L^c=\operatorname{ReLU}(\sum_{i}\alpha_k^cA^k).
    \label{3.11}
\end{equation}
\vspace{-4mm}

\hspace*{3.5mm} However, if an image contains multiple occurrences with slightly different orientations or views of the same class, several objects would fade away in the saliency map created by Grad-CAM. Moreover, due to its overlooking at significance disparity among pixels, parts of objects are rarely focused by Grad-CAM. Grad-CAM++ is proposed~\cite{chattopadhay2018grad} to replace the GAG with a weighted average of the pixel-wise gradients since the weights of pixels contribute to the final prediction by applying the following iterators over the same activation map $A^k$, $(i,j)$ and $(a,b)$:

\vspace{-4mm}
\begin{align}
    \begin{aligned}
        w_{k}^{c}=\sum_{i} \sum_{j} \alpha_{i j}^{k c} \cdot \operatorname{ReLU}\left(\frac{\partial y^{c}}{\partial A_{i j}^{k}}\right) \\
        y^{c}=\sum_{k} w_{k}^{c} \cdot \sum_{i} \sum_{j} A_{i j}^{k} \\
        \alpha_{i j}^{k c}=\frac{\frac{\partial^{2} y^{\ell}}{\left(\partial A_{i j}^{k}\right)^{2}}}{2 \frac{\partial^{2} y^{c}}{\left(\partial A_{i j}^{k}\right)^{2}}+\sum_{a} \sum_{b} A_{a b}^{k} \frac{\partial^{3} y^{c}}{\left\{\left(\partial A_{i j}^{k}\right)^{3}\right\}}}.
    \end{aligned}
\end{align}

\hspace*{3.5mm} Although, CAM variants managed to steer clear of back-propagating gradients all the way up to inputs, gradients are essentially propagated only till the final convolutional layer. Besides, Grad-CAM and Grad-CAM++ are limited to specific architectures that uses average-pooling layer to connect convolutional layers to FCLs. 
Gradient for a DNN architecture can not only be noisy but also tends to vanish due to saturation in Sigmoid or the flat zero-gradient region in ReLU. One of the consequences is that gradient of the output w.r.t input or internal layer activation may be noisy visually which causes problems in the plain SM~\cite{wang2020score}. 
\hspace*{3.5mm} These apply to both Grad-CAM and Grad-CAM++. A recent approach called Score-CAM is proposed by Haofan Wang et al.~\cite{wang2020score}. While Grad-CAM and Grad-CAM++ use the gradient information flowing into the last convolutional layer to represent the importance of each activation map, Score-CAM utilizes the importance called Channel-wise Increase of Confidence~(CIC): Score-CAM considers a convolutional layer $l$ in a model $f$, given a class of interest $c$, \cref{3.11} can be rewritten $L_{\text {Score-CAM}}^{c}$ as follows: 

\vspace{-4mm}
\begin{align}
    L_{S c o r e-C A M}^{c}=\operatorname{ReLU}\left(\sum_{k} \alpha_{k}^{c} A_{l}^{k}\right).
\end{align}
\vspace{-4mm}

\hspace*{3.5mm} where $\alpha_{k}^{c}=C\left(A_{l}^{k}\right)$ and  $C(\cdot)$ denotes the CIC score for activation map $A_{l}^{k}$. Since the weights come from the CIC score corresponding to activation maps on target class, Score-CAM gets rid of the dependence on gradient. Score-CAM can localize both single and multi objects accurately. From application perspective, Grad-CAM tends to only capture one object in the image, Grad-CAM++ and Score-CAM can localize multiple objects, but SM of Score-CAM are more focused than Grad-CAM++~\cite{wang2020score}.

\subsubsection{Layer-wise relevance propagation}
LRP~\cite{LRP1} is based on an assumption that the likelihood of a class can be traced backwards through a network to the individual layer-wise nodes of the input~\cite{LRP2}. LRP identifies important pixels by running a backward pass in the neural network. The backward pass is a conservative relevance redistribution procedure, where nodes that contribute the most to the higher-layer receive most relevance from it. First, an image $x$ is classified in a forward pass. Relevance $R_{t}^{(L)}$ is then back-propagated using deep Taylor decomposition (DTD)~\cite{DTD} to generate a relevance map $R_{LRP}$. Assuming the network has $L$ layers and for layer $l$, $1,2,...,N$ are the nodes, $1,2,..,M$ are the nodes in layer $l+ 1$. The relevance $R_{n}^{(l)}$ at node $n$ in layer $l$ is recursively defined as follows~\cite{LRP2}.  

\vspace{-4mm}
\begin{align}
    R_{n}^{(l)}=\sum_{m} \frac{a_{n}^{(l)} w_{n, m}^{+(l)}}{\sum_{n^{\prime}} a_{n^{\prime}}^{(l)} w_{n^{\prime}, m}^{+(l)}} R_{m}^{(l+1)}.
    %\label{eq:rn1}
\end{align}

The node-level relevance in case of negative values is calculated using ReLU as follows~\cite{LRP2}:

\vspace{-4mm}
\begin{align}
    R_{n}^{(l)}=\sum_{m} \frac{x_{n}^{(l)} w_{n, m}^{(l)}-b_{n}^{(l)} w_{n, m}^{+(l)}-h_{n}^{(l)} w_{n, m}^{-(l)}}{\sum_{n^{\prime}} x_{n^{\prime}}^{(l)} w_{n^{\prime}, m}^{(l)}-b_{n^{\prime}}^{(l)} w_{n^{\prime}, m}^{+(l)}-h_{n^{\prime}}^{(l)} w_{n^{\prime}, m}^{-(l+1)}}.
    \label{eq:rn_neg}
\end{align}

The output layer relevance is calculated before the back-propagation as follows~\cite{LRP2}:

\vspace{-4mm}
\begin{align}
    R_{n}^{(L)}=\left\{\begin{array}{ll}
    {z_{t}^{(L)}} & {n=t}, \\
    {0} & {\text { otherwise.}}
    \end{array}\right.
    \label{eq:rn}
\end{align}


\begin{figure}[h]
	\centering
	\includegraphics[scale=0.65]{images/lrp.png}	
    \caption[Overview of LRP for explaining model's decision]{Overview of LRP for explaining model's decision, based on LRP method by Binder et al.~\cite{binder2016layer}. First, the input image is processed , before the output/prediction is generated. Then, the output value is backpropagated layer-to-layer onto the pixel, where pixel relevances $R_p$ are visualized as heatmap.}	
	\label{fig:explain_with_lrp}
\end{figure}

\hspace*{3.5mm} Shortcomings of LRP is that it only considers the target class for the calculation, which can lead to the miss-attribution of input regions to the relevance. To tackle the issue of discriminating the target object’s class with the non-target classes, a class contrastive improvement on LRP called contrastive LRP~(CLRP) is proposed~\cite{LRP3}. 
By equally penalizing non-target classes, the relevance produced in CLRP is, however, reduced to zero, which is due to equal weighting of the non-target nodes~\cite{ LRP2}. Conversely, SGLRP is designed to take advantage of the post-softmax probabilities, where the gradient of the softmax output $\hat{y}_{t}$ is computed w.r.t, intermediate value of each output node $z_{n}$ as the relevance of the output layer $R_{n}^{(L)}$. The corresponding relevance map $R_{SGLRP}$ is then defined similar to \cref{eq:rn}, where $R_{1}^{(1)}, \ldots, R_{n}^{(1)}, \ldots, R_{N}^{(1)}$ are the relevance values at the input layer calculated by \cref{eq:rn} and \cref{eq:rn_neg}, except for the output layer relevance $R_{n}^{(L)}$, which is calculated as the gradient of softmax~\cite{LRP2}:

\vspace{-2mm}
\begin{align}
        R_{n}^{(L)}=\frac{\partial \hat{y}_{t}}{\partial z_{n}}=\left\{\begin{array}{ll}
        {\hat{y}_{t}\left(1-\hat{y}_{t}\right)} & {n=t}, \\
        {-\hat{y}_{t} \hat{y}_{n}} & {\text { otherwise.}}
    \end{array}\right.
\end{align}

\subsubsection{Decision rules-based explanations}
Due to the nested non-linear and complex structure, DNN architectures are mostly opaque and perceived as `black box' methods. They not only suffer from lack of transparency but also cannot reason about their underlying decisions. A DNN produces a prediction, which is an outcome of a bunch of mathematical expressions chained together that represent the way inner layers of an algorithm. However, using a set of rules, it possible to explain a decision intuitively to human with the ability to look up the reason for a decision. A \textit{decision rule} is a simple \textit{IF-THEN} statement consisting of a condition called antecedent and a conclusion~\cite{molnar2019interpretable}. The IF-THEN structure semantically resembles natural language and the way how human think~\cite{molnar2019interpretable}. Following are the characteristics of a decision rule: 

\begin{itemize}[noitemsep]
    \item \textbf{Structure} - follow a general structure: IF conditions are met THEN make a certain prediction. 
    \item \textbf{Number of condition} - a decision rule has at least one feature=value statement in the condition, with no upper limit and more statements can be added with an ‘AND’ operator. 
    \item \textbf{Single or multiple decision rules} - although a combination of several rules can be used to make predictions, sometimes a single decision rule is enough to explain the whole outcome.
\end{itemize}

\hspace*{3.5mm} If derived from intelligible features and the length of the condition is short, decision rules are probably the most interpretable prediction models. Suppose we have a simple model for predicting the body mass index~(BMI). Interpreting a predicted BMI with a decision rule is very easy and can be explained in layman term. For example, IF BMI $\geq$ 30~(condition), THEN you're obese~(prediction). Further explanations can be provided saying that ``an increased BMI is associated with risk for developing \textit{type-2 diabetes}, hypertension, and cardiovascular disease". 
On the contrary, even this simple concept may not be well understood by patients~\cite{post2015patient} themselves. Once we have a transparent and easily explainable AI model, decision rules can be generated using a series of IF-THEN statements. In this thesis, one of the goals is to generate human-interpretable decision rules automatically based on self-explanatory logic for all decisions. 

\subsubsection{Neuro-symbolic reasoning and challenges of current XAI methods}
Alejandro B. A, et al.~\cite{arrieta2020explainable} outline the trade-off between interpretability and accuracy, i.e., between the simplicity of the information given by the system on its internal functioning, and the exhaustiveness of this description
cognitive sciences to create objectively convincing explanations.
Explanations are better when constrictive, i.e., a good explanation is not only indicates why the model made a decision $\mathrm{X}$, but also why it made decision $\mathrm{X}$ rather than decision $\mathrm{Y}$. 

\hspace*{3.5mm}It is also explained that probabilities are not as important as causal links in order to provide a satisfying explanation. Considering that `black-box' models tend to process data in a quantitative manner, it would be necessary to translate the probabilistic results into qualitative notions containing causal links. Besides, counterfactual explanations can help the user to understand the decision of a model. In such a scenario, combining connectionist and symbolic paradigms is a promising way to address this challenge. On one hand, connectionist methods are more precise but opaque. On the other hand, symbolic methods are popularly considered less efficient, albeit they offer following features:

\begin{itemize}[noitemsep]
    \item It is possible to establish reasoning rules to allow symbolic methods to be constrictive.
    \item The use of a $\mathrm{KB}$ formalized by an ontology allows data processing in a qualitative way.
    \item Being selective is less straightforward for connectionist models than for symbolic ones.
\end{itemize}

\hspace*{3.5mm}The representation of the external reality using symbols, it seems obvious that the use of the symbolic learning paradigm is appropriate to produce an explanation. Therefore, neuro-symbolic interpretability could provide convincing explanations while keeping or improving generic performance. %An efficient explainable model enables users see the explanations that were already deduced based on the background knowledge. 
Having a semantic representation of the knowledge can help a model to have the ability to produce explanations. 

\iffalse
\subsection{Library and frameworks for interpretability}
\subsubsection{LIME}
Local Interpretable Model-agnostic Explanations~(aka. LIME) is an interpretable library to explain the predictions of any classifier or regressor. LIME explains the predictions by approximating it locally with an interpretable model. Additionally, LIME is is capable of explaining black-box classifiers, with two or more classes. 
\subsubsection{SHAP} 

\subsection{Measurements for explainability}
Unfortunately, there is little consensus on what interpretability in ML is and how to evaluate it for benchmarking. Current interpretability evaluation typically falls into two categories: 

\begin{itemize}[noitemsep]
    \item The first evaluates interpretability in the context of an application: if the system is useful in either a practical application or a simplified version of it, then it must be somehow interpretable~\cite{bhatt2020explainable}. 
    \item The second evaluates interpretability via a quantifiable proxy: a researcher might first claim that some model class—e.g. sparse linear models, rule lists, gradient boosted  trees—are interpretable and then present algorithms to optimize within that class~\cite{miller2018explanation}. 
\end{itemize}

\hspace*{3.5mm} To large extent, both evaluation approaches rely on some notion of ``you’ll know it when you see it'' or ``seeing is believing''. The notions of interpretability meet the first test of having face-validity on the correct test set of subjects: human beings. However, this is too naive that leaves many kinds of questions unanswerable, e.g., ``are all models in all defined-to-be-interpretable"? and ``are all model classes equally interpretable"? The simplistic answer to these questions would be defining some metrics for evaluating such an XAI model. Robert Hoffman et al.~\cite{hoffman2018metrics} proposed the following metrics/methods for evaluating such an XAI model, which we will try to answer in \cref{chapter:fairness}:

\begin{itemize}[noitemsep]
    \item The goodness of the explanations
    \item Whether users are satisfied by explanations
    \item How well users understand the AI systems
    \item How curiosity motivates the search for explanations
    \item Whether the user's trust on the AI is appropriate
    \item How the human-XAI work system performs.
\end{itemize}
\fi 

\section{Cancer Growth and Metastasis}
\label{cancer_growth}
Cancer is an umbrella term for a large group of lethal diseases. Till date, more than 200 types of cancer have been identified~\cite{82Tomczak}, making it the second leading cause of death worldwide~\cite{pancan}. According to a statistic from the National Cancer Institute\footnote{\url{https://www.cancer.gov/about-cancer/understanding/statistics}}, there were around 14.1 million cancer cases in 2012 in which as many as 8.8 million people died of five leading cancers of lung, liver, colorectal, stomach, and breast~\cite{stat}. In 2018, an estimated 17.35 million new cases of cancer have been diagnosed in the United States in which 609,640 people died. The number of new cancer cases per year is expected to rise to 23.6 million by 2030, which is anticipated to increase further by 70\% by 2035~\cite{71Torre}. 

\subsection{The growth of cancer}
In this section, we focus on the different types and subtypes of cancer and their growth and metastasis. The idea is to acquire some biological interpretations of the genomics data for our DSS. Cancer mainly caused when abnormal cells divide rapidly, and spread to other tissue and organs~\cite{pancan}. Cancer is often  driven by a series of genetic mutations of genes induced by selection pressures of carcinogenesis in the cells~\cite{ghazani2017assigning, baker2015cancer}. Those genes often called called marker genes ~(or biomarkers) include both onco- and tumor suppressor genes. When biomarkers over- or under-express in cancer cells as differentially expressed genes, they become uncontrollable proliferation or immorality of cancer cells~\cite{ghazani2017assigning, baker2015cancer}. 

\hspace*{3.5mm} Although the difference in the average of expression between two sample classes are often employed in many transcriptomics analyses, such difference is not the only way that a gene can be expressed differentially~\cite{xie2018adaptively}. In fact, there exist a number of regulators or mediators in cells such as transcriptional factors or miRNA, regulate a target gene in a collective way and accordingly shape a complex and heterogeneous expression pattern across inter- or intra-classes for the target gene~\cite{ghazani2017assigning, baker2015cancer}, albeit they work quite independently. Such regulatory mechanisms may account for the high biological variability where, for example, samples in one condition show a bi-modal pattern of expression versus the other condition which show a unimodal pattern of expression across samples~\cite{xie2018adaptively}. In a nutshell, following biological processes are mainly responsible for the growth of cancer: 

\begin{itemize}[noitemsep]
    \item \textbf{Minor mutations} - when a minor change happened to the DNA in the cells, our body cells correct these mistakes, by converting it into a healthy cell. However, when a mistake cannot be corrected, a cell can become cancerous. 
    \item \textbf{Major mutations} - when a minor change causes the cells to be replaced to survive instead of dying, and new cells to form when they're not needed. These extra cells can divide uncontrollably, causing growths called tumors~\cite{82Tomczak}. Besides, genetic mutations can be inherited. Cancer disrupts grow and divide process of healthy cells, which leads to abnormal growths~\cite{pancan}. 
    \item \textbf{Gene alterations} - changes in the structure of occurring genetic aberrations, such as somatic mutations, copy numbers~(CN), profiles, and different epigenetic alterations can form tumor or cancerous cells as they affect normal cell division and  growth~\cite{82Tomczak,13cancerdef,19Cruz}. 
%It’s caused by changes or mutations in DNA. DNA exists in the individual genes of every cell. It has instructions that tell the cell what functions to perform and how to grow and divide. 
\end{itemize}

\subsection{Cancer metastasis and subtypes}
When gene expression~(GE) is disrupted by cell division, environmental effects, or genetically inherited from parents, it also changes the production of different proteins, affecting normal cell behavior. Cancer cells that gone wrong no longer respond to many of the signals that control cellular growth and death. Subsequently, damaged cells start reproducing more rapidly than usual and gradually increase in the affected area by forming a tumor. Intermittently, such tumors turn into a type of cancer~\cite{zuo2019identification,24Podolsky}. Tumors can cause a variety of health problems, depending on where they grow in the body~\cite{zuo2019identification}. They can grow large and cause problems when they press against neighboring organs and tissue, although not all tumors are cancerous. During the early stages, tumors are typically benign\footnote{Benign tumors are noncancerous and do not spread to nearby tissues~\cite{o2010unit}} and remain confined within the normal boundaries of a tissue. When tumor grow and become malignant, they can break through these boundaries and invade adjoining tissues~\cite{o2010unit} and can invade other parts of the body~\cite{13cancerdef}. 

\hspace*{3.5mm} Cancers are named for the area in which they begin and the type of cell they are made of~\cite{19Cruz}, even if they spread to other parts of the body, e.g., a cancer that begins in the lungs and spreads to the liver is still called lung cancer. This process is called metastasis is how the cancer spreads to different parts of the body from where it started growing~\cite{13cancerdef}. Some cancer cells can also migrate through the bloodstream or lymphatic system to distant areas of the body. When this spread over, an oncologist refers saying the cancer has ``metastasized"~\cite{22Ding}. Cancers that have metastasized are considered more fatal and advanced than those that have not, hence are harder to treat. On the other hand, cancer subtypes describes the smaller groups that a type of cancer can be divided into, based on certain characteristics of the cancer cells. Besides, there are several clinical terms used to describe certain types of cancer\footnote{\url{https://www.healthline.com/health/cancer\#types}}:

%\vspace{-4mm}
\begin{itemize}[noitemsep]
    \item \textbf{Carcinoma} is a cancer that starts in the skin or the tissues that line other organs.
    \item \textbf{Sarcoma} is a cancer of connective tissues, e.g., bones, muscles, and blood vessels.
    \item \textbf{Leukemia} is a cancer of bone marrow, which creates blood cells.
    \item \textbf{Lymphoma} and myeloma are cancers of the immune system.
\end{itemize} 
%\vspace{-4mm}

\hspace*{3.5mm} Although more than 200 types have been reported, cancer of bladder, breast, colon and rectal, endometrial, kidney, leukemia, liver, lung, melanoma, pancreatic, prostate, and thyroid are most common~\cite{71Torre}. 

\subsection{Genomics Data-based Diagnosis of Cancer}
Discovery of important biomarkers is a significant step towards understanding the molecular mechanisms of carcinogenesis and prognosis of certain cancer type. In cancer research, multimodal data and clinical outcomes need to be processed for understanding its genetic and epigenetic causes before diagnosis and recommending appropriate treatment. By acquiring insights from the omics data, treatment can be focused on preventive measures. Such multimodal data include omics, imaging~(e.g., histology and radiology), pathological reports, and clinical outcomes~\cite{22Ding, 23Zheng}. Genomics data such as somatic mutations, copy number variations~(CNVs), DNA methylation~(DM), gene expression~(GE), miRNA expression~(MR), and Isoform expression~(IEx) are more widely used~\cite{yates, ncbi1,ncbi2, kozomara1,kozomara2}.  

\hspace*{3.5mm} As the importance of genetic knowledge in cancer treatment is increasingly addressed~\cite{15Wu}, several projects have emerged. In particular, the next generation sequencing~(NGS) is playing a key role in therapeutic decision making for the cancer prognosis and treatment~\cite{jha2017towards}, leveraging a massive amount of sequencing omics. The cancer genome atlas~(TCGA)~\cite{tomczak2015cancer} is best known for omics data and is a collection of bio-molecules inside living things such as genomics, metabolomics, and proteomics. However, omics data are generated from multiplatform and heterogeneous sources, which needs to be analyzed to make clinical decisions, where both multimodality and heterogeneity impose great challenges to bioinformatics tools and computational algorithms~\cite{karimACCESS2019,karimBIB2019}. 
%Now that we already know about genomics data, up next we discuss each data types. 

\subsubsection{Gene and miRNA expression}
Due to  gene alterations and abnormal behaviors of genes, GE can be disrupted by cell division or environmental effects, or genetically inherited from parents. Changes in GE sometimes change the production of different proteins, affecting normal cell behavior. These damaged cells start reproducing more rapidly than usual and gradually increase in the affected area by forming a tumor. Intermittently, such tumors turn into a type of cancer~\cite{zuo2019identification,24Podolsky}. GE quantification data contains the amount of expression per gene for each patient based on ensembl gene identifiers, while miRNA expression quantification data contains the amount of miRNA expression for each patient based on miRNA identifiers. Following are different types of available data in TCGA, for each patient~\cite{ncbi1,ncbi2}: 

\begin{itemize}[noitemsep]
    \item  \textbf{Masked somatic mutations} - description of masked of SNV or SNP in which the data is categorized based on chromosome, gene, and position of SNP in base pair~(BP). 
    \item \textbf{Copy number segment} - amount of copy numbers based on on chromosome and position in BP. 
    %\item \textbf{Masked copy number segment} -amount of masked CNV. 
    \item \textbf{DNA methylation} - amount of methylated DNA per CpG probe identifiers. 
    \item \textbf{Gene expression} - amount of expression per gene based on ensemble gene identifiers. 
    \item \textbf{miRNA expression} - amount of miRNA expression based on miRNA identifiers. 
    \item \textbf{DNA methylation} -a amount of methylated DNA per CpG probe identifiers. 
    \item \textbf{Isoform expressions} - amount of isoform expression based on miRNA identifiers. 
    \item \textbf{Clinical} - clinical outcomes and records. 
\end{itemize}

\subsubsection{Copy number variations}
\label{sec:cnv_data}
Copy number variations~(CNVs) are gene or genomic regions that appear in different number of copies in different individuals or even in different cells of the same individual, where copy numbers can vary across individual by several thousand. About 5-12\% of the human genome, including thousands of genes, may be variable in copy number, and this variation can be de novo deletions or duplications of the genome or inherited from the parents by healthy individuals~\cite{ostrovnaya2010classification}, ranging in size from 100 bp to 3 Mb~\cite{zhang2006development}. Although the significance is not fully understood, it is likely that CNVs are responsible for a considerable proportion of phenotypic variation~\cite{ostrovnaya2010classification}. Such variations may lead to changes in gene dosage and expression~\cite{diskin2009copy}. Approximately 179,450 human CNVs have been reported in the Database of Genomic Variants~\cite{iafrate2004detection,zhang2006development}. Although these reported CNVs are substantially fewer reported than SNPs, it is estimated that more than 30\% of the human genome is covered by at least one CNV~(compared to the $<1$\% covered by SNPs). Due to the CN changes in DNA segments, gene expression is changed by disrupting coding sequences, perturbing long-range gene regulations or altering gene dosages~\cite{37Yang}. CNVs result in variations in gene expressions and abnormalities in the human phenotypes~\cite{18Chen}. 
%Thus, CNVs are hypothesized to be of functional significance. These changes in gene expressions are responsible for different phenotypic variations or disease such as disabilities, diabetes, cancer, obesity, and autism spectrum disorder~\cite{38Buckland, 39Nguyen, 40McCarroll}. 
%CNVs are also associated with other complex disease susceptibility. For example, changes in gene 3p25 and 2p24.3 found to be responsible for prostate cancer aggressiveness~\cite{43Liu, 44Thean}; changes in gene expression of the BRCA group are considered to be responsible for breast and ovarian cancer~\cite{45Petrij, 46Montagna}; deletions of GSTM1 and GSTT1 are proven to be responsible for decreasing the five-year cancer survival rate in Dutch people who have prostate cancer and bladder cancer~\cite{48Diskin, 31Park}.

\iffalse
\begin{table*}[h]
	\begin{center}
		\caption{different types of publicly available genomics data in TCGA}
		\label{tab:tcga_data_all_des}
		%\resizebox{\textwidth}{!}{
		\vspace{-2mm} 
			\begin{tabular}{| l | l |} 
				\hline
				\multicolumn{1}{|c|}{\textbf{Data type}} & \multicolumn{1}{c|}{\textbf{Description}} \\ 
				\hline
				Masked somatic mutations & \multicolumn{1}{p{10cm}|}{Description of masked (without germline mutation) of single nucleotide variation~(SNV) or Single nucleotide polymorphism~(SNP) for each patient in which the data is categorized based on chromosome, gene, position of SNP in base pair.} \\
				\hline
				Copy number segment & \multicolumn{1}{p{10cm}|}{Amount of copy numbers for each patient based on chromosome, and position of CNV in base pair.} \\
				\hline
				Masked copy number segment & \multicolumn{1}{p{10cm}|}{Amount of masked (without germline mutation) CNV for the patients.} \\
				\hline
				DNA Methylation & \multicolumn{1}{p{10cm}|}{Amount of methylated DNA for each patient per CpG probe identifiers~\cite{ncbi1,ncbi2}.} \\
				\hline
				Gene expressions & \multicolumn{1}{p{10cm}|}{Amount of expression per gene for each patient based on ensembl gene identifiers~\cite{yates}.} \\
				\hline
				miRNA expressions & \multicolumn{1}{p{10cm}|}{Amount of miRNA expression for each patient based on miRNA identifiers~\cite{kozomara1,kozomara2,griffiths-jones1,griffiths-jones2,griffiths-jones3}.} \\
				\hline
				Isoform expressions & \multicolumn{1}{p{8cm}|}{Amount of isoform expression for each patient based on miRNA identifiers.} \\
				\hline
				Clinical & \multicolumn{1}{p{8cm}|}{Clinical outcomes and records of the patients.} \\
				\hline
		\end{tabular}%}
	\end{center}
\end{table*}
\fi 

\hspace*{3.5mm} These became more obvious because CNVs are associated with the risk of individual cancer~\cite{cnv11,cnv12,cnv13}. For example, CNVs that play a role in genetic predisposition to pancreatic adenocarcinoma~\cite{cnv13} are associated with breast cancer risk and prognosis~\cite{cnv12} and are responsible for the spatial pattern change in colorectal cancer~\cite{cnv11}. This is because the presence of CNVs in cancer patients’ cells is abundantly high, which is very different than healthy cells. Consequently, many clinically important CNVs are outcomes of duplication or deletion of a genomic region with at least 1Kb~(or shorter) in length. The CNVs related data used in this thesis include copy number segment~(CNS), which is the amount of CNVs for each patient based on chromosome, and position of CNV in base pair. On the other hand, the masked CNS contains the amount of masked, i.e., without germline mutation CNV for each patient.

\subsubsection{DNA methylation}
An altered DNA methylation pattern can have a severe impact on the behaviour of a cell by changing the binding affinity of transcription factors~(TF) and thereby intervene in GE. By silencing genes which code information for proteins involved in DNA repair, cell cycle regulation or suppression of metastasis, DNA methylation can also support oncogenesis. The changes in the DNA methylation pattern resulting in this can either occur prior to cell mutation or following it. Subsequently, either they initiate cancer development or they support it. Since the influence of DNA methylation is generally limited to only decrease the rate at which the respective protein is created, its role in the expression of genes relevant for cancer development is known to be more of quantitative character. The DNA methylation data used in this thesis contains the amount of methylated DNA for each patient per CpG probe identifiers. 

%\subsubsection{Other genomics data}
%Other types of genomics data that are used in cancer research includes masked somatic mutation data. These data provides the description of masked, i.e., without germline mutation of Simple SNV or SNP for each patient. Data is categorized based on chromosome, gene, position of SNP in base pair. The isoform expression quantification contains the amount of isoform expression for each patient based on miRNA identifiers. %\subsubsection{Characteristics of genomics data}

\section{Characteristics of genomics data}
In order to develop a DSS for clinical setting, we need to know the data as well as the mechanism for which the diagnosis decision will be based on. First of all, some types of cancer run in certain families. However, majority cancer types are not clearly linked to the genes, but we inherit from our parents. Therefore, this thesis will not consider family history records. Instead, we will mostly use the omics data and clinical information from one of the well-known project called PanCancerAtlas~(PCAt). PCAt covers multi-platform genomic measurements from 33 different cancer types~\cite{pancan}. 
%Therefore, we will use the PCAt data, due to availability and quality. 
The expression datasets in PCAt have unique characteristics that are different from text, images, or relational datasets. In particular, genomics data from PCAt show the following characteristics~\cite{lu2003cancer}:

\vspace{-2mm}
\begin{itemize}[noitemsep]
    \item \textbf{High dimensionality} - up to tens of thousands of genes.
    \item \textbf{Very small sample size} - less than $100$.
    \item \textbf{Very sparse feature space} as majority of genes are not responsible for cancer, i.e., not related to cancer types classification.
\end{itemize}
\vspace{-2mm}

\hspace*{3.5mm} With such a huge feature space, even a robust classifiers built upon, would prone to overfitting, due to small sample size~\cite{lu2003cancer}. Since most genes are known to be irrelevant for class distinction, their inclusion would not only introduce noise and confuse the classifiers, but also increase the computation time~\cite{lu2003cancer}. Therefore, feature selection and ranking of biologically meaningful biomarkers prior to classification would help in alleviating these problems. Nevertheless, with the `noise' from the irrelevant genes removed, the biological information hidden within will be less  obstructed~\cite{lu2003cancer}. 
%With high-throughput technologies such as next-generation sequencing~(NGS), cancer-specific genetic profiling is now possible. The entire genome sequencing data using NGS can be used to identify similar genetic mutations and genetic variations associated with different tumors. %\subsection{Data availability}
%As mentioned, different types of genomics data are used in cancer research, including somatic mutations, CNV, DNA methylation~(DM), gene expression~(GE), miRNA expression~(MR), and Isoform expression~(IEx), along with clinical outcomes. 
The largest uniformly processed cancer genomic data provided is TCGA, giving over 20,000 tumor and normal samples, where each step in the genome characterization pipeline generate following data points\footnote{\url{https://www.cancer.gov/about-nci/organization/ccg/research/structural-genomics/tcga/using-tcga/types}}:

\begin{itemize}[noitemsep]
    \item Clinical information, such as treatment information, survival data, etc. 
    %Besides, pathology reports were collected for selected sampels. 
    \item Molecular analyte metadata,e.g., sample portion weight.
    \item Molecular characterization data, e.g., gene expression values.
    %\item Patient phenotype and demographic information~(e.g., age, height, race, etc.). 
    \item Demographic information for the characterization of the patient by means of segementing the population, e.g., characterization by age, sex, or race\footnote{\url{https://docs.gdc.cancer.gov/Data_Dictionary/viewer/\#?view=table-entity-list&anchor=clinical}}.
    \item Diagnosis data from the investigation, analysis and recognition of the presence and nature of disease, condition, or injury from expressed signs and symptoms. 
    \item Family history - record of a patient's background regarding cancer events of blood relatives.
\end{itemize}

\section{Chapter Summary}
In this chapter, we provide preliminaries and several aspects required for the foundation of an AI DSS. We covered basics of decision support systems, machine learning, deep learning and neural networks, hyperparameter tuning, representation learning, neural ensemble methods, explainability and interpretability, different interpretable methods, and the mechanism of cancer growth and metastasis. 
It is now understandable that cancer is not only a lethal disease, but also tremendously complex to diagnose and treat. Therefore, not only recommending accurate diagnosis, but also knowing the biological mechanisms is important, e.g., oncogenes in order to provide subsequent treatments. 

\hspace*{3.5mm} Nevertheless, as research suggested~\cite{alirezaie2019semantic} that introducing semantics into deep learning model through ontological-reasoning will likely start a new era towards XAI, where the ML model reads input data, generates the predictions, observe the results, use the existing knowledge from the KB as a semantic reasoner, and produces new knowledge to provide more human-interpretable answers and explanations. The benefit is that the model learns not only from the data but also from explicit and encoded prior knowledge, which not only helps avoiding making similar mistakes in successive iterations, but also helps reducing the biases. 
Eventually, if an explainable DSS system can explain its reasoning, we can verify whether that reasoning is sound with respect to these auxiliary criteria for real clinical setting. 

\hspace*{3.5mm}In the next chapter, we start our explainable journey towards developing a DSS to leverage the cancer diagnosis and making the decision explainable and trustworthy.~We will use CNV data~(i.e., single modality) and investigate whether it effectively helps to predict cancer types. We will employ neural ensemble method to make the prediction pipeline robust. 